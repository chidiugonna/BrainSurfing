{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MRI Brain Surfing An introduction to Surface formats and Surface analysis methods for MR brain images written to support Computing for Neuroimagers (SLHS 497/597) run at the University of Arizona in Fall 2020. The GitHub repository that supports this documentation is available here Links to the documentation for the lessons are provided below: Practicum 8.1 : Surface-Based Data Formats Practicum 9.1 : Surface-Based Analysis","title":"Home"},{"location":"#mri-brain-surfing","text":"An introduction to Surface formats and Surface analysis methods for MR brain images written to support Computing for Neuroimagers (SLHS 497/597) run at the University of Arizona in Fall 2020. The GitHub repository that supports this documentation is available here Links to the documentation for the lessons are provided below: Practicum 8.1 : Surface-Based Data Formats Practicum 9.1 : Surface-Based Analysis","title":" MRI Brain Surfing "},{"location":"about/","text":"Document History Date Author Document Action 2020-09-06 Chidi Framework established 2020-09-29 Chidi Revisions to Practicum 8 completed 2020-10-18 Chidi First Draft of Practicum 9 2020-11-02 Chidi Corrections to Practicum 8 and 9","title":"About"},{"location":"about/#document-history","text":"Date Author Document Action 2020-09-06 Chidi Framework established 2020-09-29 Chidi Revisions to Practicum 8 completed 2020-10-18 Chidi First Draft of Practicum 9 2020-11-02 Chidi Corrections to Practicum 8 and 9","title":"Document History"},{"location":"surfanalysis/analysisref/","text":"Pipelines for Surface-Based Analysis There are many approaches that can be taken to perform Surface-Based neuroimaging analysis and the steps used will depend on a number of conditions e.g. what image modalities you have access to, the quality of the data at your disposable and of course what research question you are hoping to answer. Surface-Based analysis in a nutshell Assuming that sufficiently high quality data has been collected on a brain structure that will benefit from surface-based analysis then the first step in the analysis process is to generate the surface meshes of each individual subject from the anatomical data in a surface data format like GIFTI. Once the surface model is created then data from a variety of modalities e.g. fMRI, perfusion data, Susceptibility Weighted data etc and derivatives of these modalities e.g. cortical thickness etc can be projected onto the surface as surface overlays or layers . Once on the surface then additional processing can be performed using valid surface-based geodesic location information . For example smoothing of fmri volumes can be performed which avoid mixing signals across sulcal banks. For group analysis it is necessary to register subjects to a common template for direct comparison. Surface-based registration to a common template can thus be performed on the surface meshes and the registration transform implictly applied to the overlays for inter-subject comparison. Finally statistical analysis can be performed on the registered volumes. For simple analysis that doesn't require knowlesge of surface locations then direct analysis using MATLAB is feasible. If the data is in CIFTI format with both volume and surface data then a fakenifti can be created and analysed using wb_command. Analysis that requires awareness of spatial location on the surface then this can be challenging to implement on one's own. The number of tools that can help with this is growing and includes command line interfaces to FILM and FLAME, as well as PALM. Surface-Based analysis pre-conditions Structures with folded cortex Surface-based methods are most appropriate for analysis of the tightly-folded cerebral cortex and cerebellar cortex which seem to demonstrate structural and functional differentiation along the 2D surface of the cortex. High Spatial Resolution In order to take advantage of surface-based methods a high-resolution anatomical image like a T1w image with good grey, white and CSF contrast is required. For the Cerebral cortex a 1 mm T1w image is usually sufficient for creating the surface meshes that are foundational to all the downstream processing steps. For the much thinner and much more highly folded cerebellar cortex then it has been suggested that a spatial resolution of about 0.2 mm [Diedrichsen and Zotow, 2015] is required for full unfolding. If functional MRI is going to be studied on the surface then to avoid partial volume effects between vertices on opposite banks of a sulcus then at least 2mm resolution [Smith et al, 2013] is required for the cerebral cortex. Tools for Surface Mesh creation Freesurfer Use version 7.1 over version 7.0.0. As the latter has been recalled . Available as a docker image from Docker Hub Surface-Based Analysis Pipelines using Freesurfer HCP Pipelines The HCP pipelines were developed by the Human Connectome Project to explore the structural and functional connectivity of 1200 healthy young adults using high resolution MRI data which includes T1w (0.7mm), T2w (0.7mm) and fMRI (2mm) data Latest release is v4.2.0 . Download and extract the zip file to use. Fmriprep versions of fmriprep available on Docker Hub . Avoid buggy versions as far as possible. Look at Read me and also at releases to determine version to take. Will take most stable release which is 20.1.3 docker pull poldracklab/fmriprep:20.1.3 Ciftify Ciftify facilitates surface generation on legacy MRI data using the HCP approach which was developed for high quality data and their own very specific acquisition protocol (Field Maps, Single Band Reference, High resolution T2W images etc..) The latest version of Ciftify can also run fmriprep to produce required outputs and transform your BIDS data into HCP-type data. The provided docker version of ciftify uses fmriprep version 1.3.2. It is recommended to run fmriprep first on your data using the latest fmriprep and then run ciftify with the right flags. docker pull tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 Practical Pipeline Implementations Freesurfer + fmriprep This option might be necessary if you want to use a version of freesurfer that is not bundled as part of the fmriprep image or if you have already preprocesed your surfaces with freesurfer. It is possibleto create a docker version of fmriprep with a different version of freesurfer but it will be important to flag this in the citation of any work you do. Run freesurfer on standard 1mm resolution T1w data. T2w data can also be provided if available to improve the accuracy of the pial layer. recon-all -all -subject $SUBJECT -i $IMAGE -T2 $T2IMAGE -T2pial If the T2 images were omitted in a first run, then they can be subsequently run as follows recon-all -all -subject $SUBJECT -i $IMAGE -T2 $T2IMAGE -T2pial -autorecon3 Using Docker you can point to your license using the FS_LICENSE environment variable Before running fmriprep then ensure that $OUTPUTDIR for your fmriprep analysis contains the freesurfer folders of your subject and fsaverage for your version of freesurfer. cifti outputs are generated using --cifti-output docker run -v /tmp:/tmp -v /media:/media poldracklab/fmriprep:20.1.3 --participant_label 001 --cifti-output --nthreads 8 -w /media/INFO/SurfaceProcessing/fmriprep/work --use-aroma --fs-license-file /mnt/license.txt /media/INFO/NEURODATA/UA/BIDS /media/INFO/SurfaceProcessing/fmriprep/output participant Fmriprep Fmriprep can also be set up to run the freesurfer step as part of its run. It runs freesurver v6.0.0 without the skull stripping step to perform initial basic reconstruction. It then utilizes a previously calculated brain mask to complete surface reconstruction. Simply running the docker step above will thus run freesurfer and fmriprep to create the cifti files in Ciftify Ciftify can be run on fMRI volumes for subjects that have had anatomical surfaces created by Freesurfer. Ciftify can also be run on fmriprep outputs. will need to run fmriprep with --output-spaces for cifti_fmri_subject - it looks like anat is the important one - will need to test. But this reference might be important --output-spaces MNI152NLin6Asym:res-2 anat MNI152NLin2009cAsym The only difference between Ciftify CIFTI files and the fmriprep versions is the use of MSM-sulc in Ciftify docker run -v /tmp:/tmp -v /media:/media tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /media/INFO/NEURODATA/UA/BIDS /media/INFO/SurfaceProcessing/ciftify_fmriprep/output participant --participant_label=001 --n_cpus=8 --fmriprep-workdir=/media/INFO/SurfaceProcessing/ciftify_fmriprep/work --fs-license=/mnt/license.txt Found this to be problematic - much better it seems to run ciftify_recon_all and cifti_fmri_subject directly. docker run --entrypoint ciftify_recon_all -v /tmp:/tmp -v /media:/media tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 \\ --resample-to-T1w32k \\ --fs-subjects-dir /path/to/freesurfer/outputs \\ --hcp-data-dir /path/for/hcp/outputs Subject001 Ciftify works seamlessly on the fmri files in T1w space _space-T1w_ There are issues running Ciftify using the --already-in-MNI flag on both the MNI152NLin6Asym:res-2 outputs and the then the MNI152NLin2009cAsym outputs. The volume ROI.2.nii.gz used for creating the dense timeseries is in MNI152NLin6Asym space (91,109,91) at 2mm isotropic and but it's sform is slightly different and so there is an error. The MNI152NLin2009cAsym has completely different dimensions (129,153,87) from the ROI2 file, has been resliced at the same dimensions as the original func (1.5, 1.5, 2.5) and also has different sform and so a lot of work to do there too. Note Most of the problems with docker come down to correct binding between the external OS and the docker container using the -v parameter. Confirm that the path you are using is indeed correctly bound. If you are accessing a path via a symlink then make sure that the sym link path is what has been used Freesurfer Pipeline with High Resolution anatomicals With High resolution anatomicals of < 1mm3 then recon-all can be run with the -hires flag. An $EXPERT_FILE can also be passed with the -expert option to set the number of iterations used during the inflation. recon-all -all -s $SUBJECT -hires -i $IMAGE -expert $EXPERT_FILE An alternative approach that was developed for the HCP workflow can also be used. This uses the -conf2hires flag. This approach peforms the surface modelling in stages. With this approach however the volumetric files will be at 1mm resolution. To have both surfaces and volumes at the sub-millimeter resolution then use the -hires flag. recon-all -all -s $SUBJECT -conf2hires -i $IMAGE -expert $EXPERT_FILE Pipeline with HCP-style data The HCP Pipelines can be directly adapted to run on this type of data. References Van Essen, D. C. (2002). Surface\u2010based atlases of cerebellar cortex in the human, macaque, and mouse. Annals of the New York Academy of Sciences, 978(1), 468-479. Coalson, T. S., Van Essen, D. C., & Glasser, M. F. (2018). The impact of traditional neuroimaging methods on the spatial localization of cortical areas. Proceedings of the National Academy of Sciences, 115(27), E6356-E6365. Diedrichsen, J., & Zotow, E. (2015). Surface-based display of volume-averaged cerebellar imaging data. PloS one, 10(7), e0133402. Smith, S. M., Beckmann, C. F., Andersson, J., Auerbach, E. J., Bijsterbosch, J., Douaud, G., ... & Kelly, M. (2013). Resting-state fMRI in the human connectome project. Neuroimage, 80, 144-168. Sereno, M. I., Diedrichsen, J., Tachrount, M., Testa-Silva, G., d\u2019Arceuil, H., & De Zeeuw, C. (2020). The human cerebellum has almost 80% of the surface area of the neocortex. Proceedings of the National Academy of Sciences, 117(32), 19538-19543. Dickie, E. W., Anticevic, A., Smith, D. E., Coalson, T. S., Manogaran, M., Calarco, N., ... & Voineskos, A. N. (2019). ciftify: A framework for surface-based analysis of legacy MR acquisitions. Neuroimage, 197, 818-826. Glasser, M. F., Sotiropoulos, S. N., Wilson, J. A., Coalson, T. S., Fischl, B., Andersson, J. L., ... & Van Essen, D. C. (2013). The minimal preprocessing pipelines for the Human Connectome Project. Neuroimage, 80, 105-124.","title":"Pipelines for Surface-Based Analysis"},{"location":"surfanalysis/analysisref/#pipelines-for-surface-based-analysis","text":"There are many approaches that can be taken to perform Surface-Based neuroimaging analysis and the steps used will depend on a number of conditions e.g. what image modalities you have access to, the quality of the data at your disposable and of course what research question you are hoping to answer.","title":"Pipelines for Surface-Based Analysis"},{"location":"surfanalysis/analysisref/#surface-based-analysis-in-a-nutshell","text":"Assuming that sufficiently high quality data has been collected on a brain structure that will benefit from surface-based analysis then the first step in the analysis process is to generate the surface meshes of each individual subject from the anatomical data in a surface data format like GIFTI. Once the surface model is created then data from a variety of modalities e.g. fMRI, perfusion data, Susceptibility Weighted data etc and derivatives of these modalities e.g. cortical thickness etc can be projected onto the surface as surface overlays or layers . Once on the surface then additional processing can be performed using valid surface-based geodesic location information . For example smoothing of fmri volumes can be performed which avoid mixing signals across sulcal banks. For group analysis it is necessary to register subjects to a common template for direct comparison. Surface-based registration to a common template can thus be performed on the surface meshes and the registration transform implictly applied to the overlays for inter-subject comparison. Finally statistical analysis can be performed on the registered volumes. For simple analysis that doesn't require knowlesge of surface locations then direct analysis using MATLAB is feasible. If the data is in CIFTI format with both volume and surface data then a fakenifti can be created and analysed using wb_command. Analysis that requires awareness of spatial location on the surface then this can be challenging to implement on one's own. The number of tools that can help with this is growing and includes command line interfaces to FILM and FLAME, as well as PALM.","title":"Surface-Based analysis in a nutshell"},{"location":"surfanalysis/analysisref/#surface-based-analysis-pre-conditions","text":"","title":"Surface-Based analysis pre-conditions"},{"location":"surfanalysis/analysisref/#structures-with-folded-cortex","text":"Surface-based methods are most appropriate for analysis of the tightly-folded cerebral cortex and cerebellar cortex which seem to demonstrate structural and functional differentiation along the 2D surface of the cortex.","title":"Structures with folded cortex"},{"location":"surfanalysis/analysisref/#high-spatial-resolution","text":"In order to take advantage of surface-based methods a high-resolution anatomical image like a T1w image with good grey, white and CSF contrast is required. For the Cerebral cortex a 1 mm T1w image is usually sufficient for creating the surface meshes that are foundational to all the downstream processing steps. For the much thinner and much more highly folded cerebellar cortex then it has been suggested that a spatial resolution of about 0.2 mm [Diedrichsen and Zotow, 2015] is required for full unfolding. If functional MRI is going to be studied on the surface then to avoid partial volume effects between vertices on opposite banks of a sulcus then at least 2mm resolution [Smith et al, 2013] is required for the cerebral cortex.","title":"High Spatial Resolution"},{"location":"surfanalysis/analysisref/#tools-for-surface-mesh-creation","text":"","title":"Tools for Surface Mesh creation"},{"location":"surfanalysis/analysisref/#freesurfer","text":"Use version 7.1 over version 7.0.0. As the latter has been recalled . Available as a docker image from Docker Hub","title":"Freesurfer"},{"location":"surfanalysis/analysisref/#surface-based-analysis-pipelines-using-freesurfer","text":"","title":"Surface-Based Analysis Pipelines using Freesurfer"},{"location":"surfanalysis/analysisref/#hcp-pipelines","text":"The HCP pipelines were developed by the Human Connectome Project to explore the structural and functional connectivity of 1200 healthy young adults using high resolution MRI data which includes T1w (0.7mm), T2w (0.7mm) and fMRI (2mm) data Latest release is v4.2.0 . Download and extract the zip file to use.","title":"HCP Pipelines"},{"location":"surfanalysis/analysisref/#fmriprep","text":"versions of fmriprep available on Docker Hub . Avoid buggy versions as far as possible. Look at Read me and also at releases to determine version to take. Will take most stable release which is 20.1.3 docker pull poldracklab/fmriprep:20.1.3","title":"Fmriprep"},{"location":"surfanalysis/analysisref/#ciftify","text":"Ciftify facilitates surface generation on legacy MRI data using the HCP approach which was developed for high quality data and their own very specific acquisition protocol (Field Maps, Single Band Reference, High resolution T2W images etc..) The latest version of Ciftify can also run fmriprep to produce required outputs and transform your BIDS data into HCP-type data. The provided docker version of ciftify uses fmriprep version 1.3.2. It is recommended to run fmriprep first on your data using the latest fmriprep and then run ciftify with the right flags. docker pull tigrlab/fmriprep_ciftify:v1.3.2-2.3.3","title":"Ciftify"},{"location":"surfanalysis/analysisref/#practical-pipeline-implementations","text":"","title":"Practical Pipeline Implementations"},{"location":"surfanalysis/analysisref/#freesurfer-fmriprep","text":"This option might be necessary if you want to use a version of freesurfer that is not bundled as part of the fmriprep image or if you have already preprocesed your surfaces with freesurfer. It is possibleto create a docker version of fmriprep with a different version of freesurfer but it will be important to flag this in the citation of any work you do. Run freesurfer on standard 1mm resolution T1w data. T2w data can also be provided if available to improve the accuracy of the pial layer. recon-all -all -subject $SUBJECT -i $IMAGE -T2 $T2IMAGE -T2pial If the T2 images were omitted in a first run, then they can be subsequently run as follows recon-all -all -subject $SUBJECT -i $IMAGE -T2 $T2IMAGE -T2pial -autorecon3 Using Docker you can point to your license using the FS_LICENSE environment variable Before running fmriprep then ensure that $OUTPUTDIR for your fmriprep analysis contains the freesurfer folders of your subject and fsaverage for your version of freesurfer. cifti outputs are generated using --cifti-output docker run -v /tmp:/tmp -v /media:/media poldracklab/fmriprep:20.1.3 --participant_label 001 --cifti-output --nthreads 8 -w /media/INFO/SurfaceProcessing/fmriprep/work --use-aroma --fs-license-file /mnt/license.txt /media/INFO/NEURODATA/UA/BIDS /media/INFO/SurfaceProcessing/fmriprep/output participant","title":"Freesurfer + fmriprep"},{"location":"surfanalysis/analysisref/#fmriprep_1","text":"Fmriprep can also be set up to run the freesurfer step as part of its run. It runs freesurver v6.0.0 without the skull stripping step to perform initial basic reconstruction. It then utilizes a previously calculated brain mask to complete surface reconstruction. Simply running the docker step above will thus run freesurfer and fmriprep to create the cifti files in","title":"Fmriprep"},{"location":"surfanalysis/analysisref/#ciftify_1","text":"Ciftify can be run on fMRI volumes for subjects that have had anatomical surfaces created by Freesurfer. Ciftify can also be run on fmriprep outputs. will need to run fmriprep with --output-spaces for cifti_fmri_subject - it looks like anat is the important one - will need to test. But this reference might be important --output-spaces MNI152NLin6Asym:res-2 anat MNI152NLin2009cAsym The only difference between Ciftify CIFTI files and the fmriprep versions is the use of MSM-sulc in Ciftify docker run -v /tmp:/tmp -v /media:/media tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /media/INFO/NEURODATA/UA/BIDS /media/INFO/SurfaceProcessing/ciftify_fmriprep/output participant --participant_label=001 --n_cpus=8 --fmriprep-workdir=/media/INFO/SurfaceProcessing/ciftify_fmriprep/work --fs-license=/mnt/license.txt Found this to be problematic - much better it seems to run ciftify_recon_all and cifti_fmri_subject directly. docker run --entrypoint ciftify_recon_all -v /tmp:/tmp -v /media:/media tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 \\ --resample-to-T1w32k \\ --fs-subjects-dir /path/to/freesurfer/outputs \\ --hcp-data-dir /path/for/hcp/outputs Subject001 Ciftify works seamlessly on the fmri files in T1w space _space-T1w_ There are issues running Ciftify using the --already-in-MNI flag on both the MNI152NLin6Asym:res-2 outputs and the then the MNI152NLin2009cAsym outputs. The volume ROI.2.nii.gz used for creating the dense timeseries is in MNI152NLin6Asym space (91,109,91) at 2mm isotropic and but it's sform is slightly different and so there is an error. The MNI152NLin2009cAsym has completely different dimensions (129,153,87) from the ROI2 file, has been resliced at the same dimensions as the original func (1.5, 1.5, 2.5) and also has different sform and so a lot of work to do there too. Note Most of the problems with docker come down to correct binding between the external OS and the docker container using the -v parameter. Confirm that the path you are using is indeed correctly bound. If you are accessing a path via a symlink then make sure that the sym link path is what has been used","title":"Ciftify"},{"location":"surfanalysis/analysisref/#freesurfer-pipeline-with-high-resolution-anatomicals","text":"With High resolution anatomicals of < 1mm3 then recon-all can be run with the -hires flag. An $EXPERT_FILE can also be passed with the -expert option to set the number of iterations used during the inflation. recon-all -all -s $SUBJECT -hires -i $IMAGE -expert $EXPERT_FILE An alternative approach that was developed for the HCP workflow can also be used. This uses the -conf2hires flag. This approach peforms the surface modelling in stages. With this approach however the volumetric files will be at 1mm resolution. To have both surfaces and volumes at the sub-millimeter resolution then use the -hires flag. recon-all -all -s $SUBJECT -conf2hires -i $IMAGE -expert $EXPERT_FILE","title":"Freesurfer Pipeline with High Resolution anatomicals"},{"location":"surfanalysis/analysisref/#pipeline-with-hcp-style-data","text":"The HCP Pipelines can be directly adapted to run on this type of data.","title":"Pipeline with HCP-style data"},{"location":"surfanalysis/analysisref/#references","text":"Van Essen, D. C. (2002). Surface\u2010based atlases of cerebellar cortex in the human, macaque, and mouse. Annals of the New York Academy of Sciences, 978(1), 468-479. Coalson, T. S., Van Essen, D. C., & Glasser, M. F. (2018). The impact of traditional neuroimaging methods on the spatial localization of cortical areas. Proceedings of the National Academy of Sciences, 115(27), E6356-E6365. Diedrichsen, J., & Zotow, E. (2015). Surface-based display of volume-averaged cerebellar imaging data. PloS one, 10(7), e0133402. Smith, S. M., Beckmann, C. F., Andersson, J., Auerbach, E. J., Bijsterbosch, J., Douaud, G., ... & Kelly, M. (2013). Resting-state fMRI in the human connectome project. Neuroimage, 80, 144-168. Sereno, M. I., Diedrichsen, J., Tachrount, M., Testa-Silva, G., d\u2019Arceuil, H., & De Zeeuw, C. (2020). The human cerebellum has almost 80% of the surface area of the neocortex. Proceedings of the National Academy of Sciences, 117(32), 19538-19543. Dickie, E. W., Anticevic, A., Smith, D. E., Coalson, T. S., Manogaran, M., Calarco, N., ... & Voineskos, A. N. (2019). ciftify: A framework for surface-based analysis of legacy MR acquisitions. Neuroimage, 197, 818-826. Glasser, M. F., Sotiropoulos, S. N., Wilson, J. A., Coalson, T. S., Fischl, B., Andersson, J. L., ... & Van Essen, D. C. (2013). The minimal preprocessing pipelines for the Human Connectome Project. Neuroimage, 80, 105-124.","title":"References"},{"location":"surfanalysis/introanalysis/","text":"Working with Surface-based Formats Surface Processing Sulc - Freesurfer Pipeline Review A really helpful review of the freesurfer pipeline is provided by Doug Greve in Part 1 and Part 2 of the 2013 Freesurfer course. The Pipeline outputs Quick review of the freesurfer pipeline detailing the created volumes and surfaces. Volumetric pipeline - from T1w > orig.mgz > T1.mgz > brainmask.mgz > aseg.mgz > wm.mgz Surface pipeline - from wm.mgz > lh.orig > lh.white > lh.pial etc.. Medial layer Each hemisphere analysed separately. Medial wall of hemosphere has non-cortical areas (subcortical structures. corpus callosum) and so to retain closed Spherical Topology the medial layer is usually enclosed in the white and pial layers and is masked out during analysis. Error Checking Initial pial and white surfaces are critical for rest of pipeline - good to learn how to apply some quality control and manual editting if possible. Also good to review volumetric steps like segmentation, skull stripping that may have been erroneous Vertex Correspondence In the same subject, vertices in each of the hemispheric surfaces correspond. So vertex 22 in lh.pial will have been created by nudging vertex 22 in the lh.white and so on. However there is no correspondence between different hemispheres or between different subjects in this native space. Note Freesurfer took the engineering choice to relax vertex alignment at the subject level to prevent bias in vertex sampling and enable all subjects to have the same sampling density regardless of anatomical variability. Overlays Some important overlays are created as part of the Freesurfer pipeline that aid with registration * curv - 3D mean curvature of the vertex - average of the principal curvatures which are maximum bending and minimum bending curvature. sulc - dot product of movement vector during inflation and the surface normal - megative if gyral, positive if sulcal - reveals large scale geometric features of the cortex better than mean curvature which has a lot of highfrequency information that is not as good for intersubject registration. thickness - Standard Spaces and Registration Mesh in native space - would be good to register all subjects to a standard space. In Freesurfer this is the fsaverage, spherical template which has been created from 40 subjects. 2D Surface Coordinate System Analogous to volumetric coordinate system in 3D. The Cortical sheet is cut in several places to turn it into a flat map. Most easilly visualzied however as a sphere using 2D coordinate system of latitude and longitude - just like the earth - this latter approach is continuous without cuts and has a value represented at each point. Spheres are obtained by inflating the surface until it is as spherical as possible with minimal distortion. The fsaverage mesh and the sulc and curv overlays can be used to register subjects to the same space for comparison. Smoothing Smoothing on the surface increase SNR and to improve intersubject registration. Smoothing kernels chosen by convention to be about 10mm - by matched filter theory would smoothing using a kernel size that matched the expected \"blob\" but not clear what this size is and it will also vary across space. Smoothing on the Surface is more accurate than in the volume as it avoids mixing signals across sulci/gyri boundaries. Quote references to demonstrate the advantages of surface smoothing - for example Doug Greve showed an asymmetric fmri study of serotonin receptors that demonstrated much more realistic activation after surface smoothing using a range of kernels. Surface-based Clustering and Multiple Comparisons Clusters in surface space are planar entities with area - this improves power as our correction for multiple comparisons is less severe as the search space is smaller - number of tests is reduced. Beyond Sulc - HCP Pipeline Review","title":"Working with Surface-based Formats"},{"location":"surfanalysis/introanalysis/#working-with-surface-based-formats","text":"","title":"Working with Surface-based Formats"},{"location":"surfanalysis/introanalysis/#surface-processing","text":"","title":"Surface Processing"},{"location":"surfanalysis/introanalysis/#sulc-freesurfer-pipeline-review","text":"A really helpful review of the freesurfer pipeline is provided by Doug Greve in Part 1 and Part 2 of the 2013 Freesurfer course.","title":"Sulc - Freesurfer Pipeline Review"},{"location":"surfanalysis/introanalysis/#the-pipeline-outputs","text":"Quick review of the freesurfer pipeline detailing the created volumes and surfaces. Volumetric pipeline - from T1w > orig.mgz > T1.mgz > brainmask.mgz > aseg.mgz > wm.mgz Surface pipeline - from wm.mgz > lh.orig > lh.white > lh.pial etc..","title":"The Pipeline outputs"},{"location":"surfanalysis/introanalysis/#medial-layer","text":"Each hemisphere analysed separately. Medial wall of hemosphere has non-cortical areas (subcortical structures. corpus callosum) and so to retain closed Spherical Topology the medial layer is usually enclosed in the white and pial layers and is masked out during analysis.","title":"Medial layer"},{"location":"surfanalysis/introanalysis/#error-checking","text":"Initial pial and white surfaces are critical for rest of pipeline - good to learn how to apply some quality control and manual editting if possible. Also good to review volumetric steps like segmentation, skull stripping that may have been erroneous","title":"Error Checking"},{"location":"surfanalysis/introanalysis/#vertex-correspondence","text":"In the same subject, vertices in each of the hemispheric surfaces correspond. So vertex 22 in lh.pial will have been created by nudging vertex 22 in the lh.white and so on. However there is no correspondence between different hemispheres or between different subjects in this native space. Note Freesurfer took the engineering choice to relax vertex alignment at the subject level to prevent bias in vertex sampling and enable all subjects to have the same sampling density regardless of anatomical variability.","title":"Vertex Correspondence"},{"location":"surfanalysis/introanalysis/#overlays","text":"Some important overlays are created as part of the Freesurfer pipeline that aid with registration * curv - 3D mean curvature of the vertex - average of the principal curvatures which are maximum bending and minimum bending curvature. sulc - dot product of movement vector during inflation and the surface normal - megative if gyral, positive if sulcal - reveals large scale geometric features of the cortex better than mean curvature which has a lot of highfrequency information that is not as good for intersubject registration. thickness -","title":"Overlays"},{"location":"surfanalysis/introanalysis/#standard-spaces-and-registration","text":"Mesh in native space - would be good to register all subjects to a standard space. In Freesurfer this is the fsaverage, spherical template which has been created from 40 subjects.","title":"Standard Spaces and Registration"},{"location":"surfanalysis/introanalysis/#2d-surface-coordinate-system","text":"Analogous to volumetric coordinate system in 3D. The Cortical sheet is cut in several places to turn it into a flat map. Most easilly visualzied however as a sphere using 2D coordinate system of latitude and longitude - just like the earth - this latter approach is continuous without cuts and has a value represented at each point. Spheres are obtained by inflating the surface until it is as spherical as possible with minimal distortion. The fsaverage mesh and the sulc and curv overlays can be used to register subjects to the same space for comparison.","title":"2D Surface Coordinate System"},{"location":"surfanalysis/introanalysis/#smoothing","text":"Smoothing on the surface increase SNR and to improve intersubject registration. Smoothing kernels chosen by convention to be about 10mm - by matched filter theory would smoothing using a kernel size that matched the expected \"blob\" but not clear what this size is and it will also vary across space. Smoothing on the Surface is more accurate than in the volume as it avoids mixing signals across sulci/gyri boundaries. Quote references to demonstrate the advantages of surface smoothing - for example Doug Greve showed an asymmetric fmri study of serotonin receptors that demonstrated much more realistic activation after surface smoothing using a range of kernels.","title":"Smoothing"},{"location":"surfanalysis/introanalysis/#surface-based-clustering-and-multiple-comparisons","text":"Clusters in surface space are planar entities with area - this improves power as our correction for multiple comparisons is less severe as the search space is smaller - number of tests is reduced.","title":"Surface-based Clustering and Multiple Comparisons"},{"location":"surfanalysis/introanalysis/#beyond-sulc-hcp-pipeline-review","text":"","title":"Beyond Sulc - HCP Pipeline Review"},{"location":"surfanalysis/prac91/","text":"Practicum 9.1 - Surface-Based Analysis Synopsis Surface-based methods provide several advantages over volume-based methods for analysing data on the cerebral cortex [Dickie et al. 2019, Coalson et al. 2018, Glasser et al. 2013]. Fundamentally, they seem to provide a more neurobiologically valid representation of the way that function is mapped along the surface of the cortex as a 2D metric [Glasser et al. 2013] . This in turn affords other downstream benefits that make surface-based approaches attractive for studying the cortex. These benefits include the facilitation of more computationally tractable algorithms for aligning cortical areas across many subjects using brain measures that lead to better alignment of functional areas [Coalson et al. 2018]. The HCP for example uses brain features like myelination and resting state fMRI to improve functional correspondence between subjects which would not be guaranteed by the restricted geometric-based measures used by Volume-Based methods. Another benefit of surface-based methods is the encouragement of constrained spatial smoothing methods that reduce the corruption of functional signal from non-related parts of the brain [Dickie et al. 2019]. These two benefits ensure that studies can be conducted accurately at high resolution across a large number of subjects and help improve the statistical power of surface based studies. Surface-based methods also allow for much better overall intuitive visualization of cortical patterns of structure and function when compared to volume-based visualization on a slice-by-slice basis. In addition, surface-based models of the cortex by nature lead to more compact and efficient representation. For example, to achieve 2mm resolution of the cortex, the Human Connectome Project uses a CIFTI file with 91282 vertices and voxels to represent gray matter in the brain. By contrast, an MNI 2mm mask requires approximately 130,000 voxels to cover the gray matter region in volumetric analysis. In addition to reduced file sizes the use of surface-based data formats easilly allows multi-modal data to be quickly aligned and matched within and across subjects greatly improving the speed and ease of analysis [Dickie et al. 2019]. Objectives This practicum will introduce you to some of the steps required for preparing MRI neuroimaging data for group surface-based statistical analysis. These steps have been copied from the Human Connectome Project's Pipelines [Glasser et al. 2013] which have been recreated by Dickie et al. 2019 for non-HCP (aka legacy) data. The scripts for this practicum are described below: 001_inflate.sh : Use wb_command to create midthickness surface and inflated surfaces. 002_register.sh : Use msm and wb_command to register 2 subject meshes to a standard surface template 003_projection.sh : Use wb_command to map an fmri volume onto a GIFTI surface mesh 004_ciftigen.sh : Use wb_command to map an fmri volume into a CIFTI file 005_smoothing.sh : Use wb_command to perform surface-based spatial smoothing on the left and right hemispheres, as well as on the subcortical volume. Note Please appreciate that the processing tasks you will undertake in this practicum are provided for demonstration purposes only and to shed some light on how these surface processing steps are roughly accomplished. For your research you will want to use one of the robust, established pipelines that not only automate the steps above but utilize the most up-to-date and accurate methods for implementing these steps. The one step that is usually left to the user to do by these pipelines is spatial smoothing . However it is advised to minimize the use of spatial smoohing , even surface-based smoothing, in order to preserve as much spatial detail as possible in the results [Coalson et al. 2018]. FMRIPrep currently performs robust pre-processing on fmri data and can output this out as a surface-based CIFTI .dtseries.nii file for surface based analysis. If you have HCP-style data then you may be able to adapt the HCP Pipelines to work with your acquisitions. Alternatively if you have data that does not meet the HCP's recommended acquisition guidelines then the Ciftify pipeline can be used to convert your data to cifti format and map it to a similar file structure as the HCP's processing pipeline outputs. And of course Freesurfer has many different tools for surface-based analysis. Many of the steps mentioned above can be accomplished with equivalent freesurfer commands. Most notable of course is that all of the above pipelines depend on freesurfer for the creation of the initial surface mesh models. In addition to these there are several other tools and methods that can provide a good starting point for developing robust surface-based processing pipelines. Downloads/Installation This practicum requires the following materials/folder setup for successful completion: A copy of the HCP's Connectome workbench for access to wb_view . You will also need the Ciftify docker image which you can obtain as follows docker pull tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 . This will allow us to run our scripts that call wb_command in a windows environment. It will also give us access to msm if you decide at a later point to fully run the registration step. Note We will be running the scripts using the docker container as shown below. For each script a command line will be provided which you will be able to copy and paste into your terminal window. Try to avoid extracting the zip folder into a root directory that has spaces in it to avoid problems with binding the current directory $PWD. If you do have to use a root directory with spaces in it then you can use double quotes around $PWD to solve this as follows -v \"${PWD}\":/mnt. docker run -v ${PWD}:/mnt --rm -it --entrypoint bash tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /mnt/ SCRIPTNAME The data and scripts for this practicum can be downloaded from the OSF site under the 09-Surface-Based-Analysis folder in a zip file practicum91.zip . Download this zip file to your local computer and extract. Your folder structure should look like this after extraction. practicum91 \u251c\u2500\u2500 001_inflate.sh \u251c\u2500\u2500 002_register.sh \u251c\u2500\u2500 003_projection.sh \u251c\u2500\u2500 004_ciftigen.sh \u251c\u2500\u2500 005_smoothing.sh \u2514\u2500\u2500 DATA \u251c\u2500\u2500 100307 \u251c\u2500\u2500 config \u251c\u2500\u2500 MMP_HCP \u251c\u2500\u2500 sub-01 \u2514\u2500\u2500 sub-02 Provenance Scripts The scripts for this practicum have been written based on an inspection of the HCP pipelines and the Ciftify pipelines . Only the essential aspects of the processing steps used in these pipelines have been provided for demonstration purposes and so these scripts are suboptimal for actual research purposes and additionally have not been tested thoroughly. Data We will be using a subset of the Rhyme Judgment data (subjects sub-01 and sub-02 ) acquired by [Xue and Poldrack 2007]. This data has also been used in an FMRIPrep exemplar pipeline for task-based analysis [Esteban et al. 2020]. The data has been processed using fmriprep, freesurfer and ciftify as described further below. We will also use flat and spherical surfaces from subject 100307 which is included as part of the Human Connectome Project's Young Adult (YA) 1200 Subject Release . The HCP places data use terms on their data. To use HCP data please sign on to the HCP website and register for free. The folder MMP_HCP contains a subset of the files released for the Multi-modal parcellation of Human Cerebral Cortex . This atlas is a comprehensive delineation of about 180 cerebral cortex areas per hemisphere obtained using the HCP's advanced analysis approach [Glasser et al. 2013, Glasser et al. 2016]. Miscellaneous HCP configuration files (atlas and registration templates, MSM Sulc registration config file etc.) are provided in config . These have been copied from the HCP Pipelines Initial processing The data was processed initially through FMRIPrep to create aligned fMRI volumes in anatomical space. FMRIprep also uses Freesurfer to generate the initial surface meshes for each subject. The fmriprep processed data was then used as the input to Ciftify to create the final dataset that you have available to you. Several of the Ciftify outputs that are not required for this practicum have been deleted to keep the download manageable. Task 1: Create midthickness, inflated and very-inflated surfaces Open a terminal and navigate to the practicum91 folder. Run the 001_inflate.sh script through the docker image as follows: docker run -v ${PWD}:/mnt --rm -it --entrypoint bash tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /mnt/001_inflate.sh The script creates the midthickness surface amended.sub-01.L.midthickness.32k_fs_LR.surf.gii in the directory practicum91/output/sub-01 . This surface lies between the pial and the white surface and is created using wb_command -surface-average . This surface is useful for projecting functional data from the volume onto the cortex. The script also creates an inflated and a very_inflated surface in the same directory using wb_command -surface-generate-inflated . These surfaces allow functional areas to be better visualized in the cortex. Note The script generates the inflated surfaces from the midthickness surface. This is similar to how it's done in the HCP pipelines but not how it's done in Freesurfer which instead uses the white surface to create the inflated surfaces. If you want to peform this on subject 02 as well then just change the subject variable on line 27 in 001_inflate.sh using a simple text editor, as follows SUB=sub-02 and rerun the docker command. Open wb_view and navigate to ./DATA/sub-01/MNINonLinear/ and load in T1w.nii.gz . Now navigate up one folder to fsaverage_LR32k and load in the two surfaces sub-01.L.pial.32k_fs_LR.surf.gii and sub-01.L.white.32k_fs_LR.surf.gii Now load in the midthickness surface you created amended.sub-01.L.midthickness.32k_fs_LR.surf.gii from ./output/sub-01 Click on the volume view and select the Vol/Surf Tab . Assign each drop down in the File column to one of the three different overlays and assign each a different color. Notice that your midthickness surface is indeed between the pial and the white surface. You may have to zoom in to see this more clearly. Now we will see how the inflated and very_inflated surfaces better help us visualize cortical areas. We will also look at the flat surface and a spherical surface that are provided by the HCP's pipelines. In wb_view now load in the inflated amended.sub-01.L.inflated.32k_fs_LR.surf.gii and very_inflated surfaces amended.sub-01.L.very_inflated.32k_fs_LR.surf.gii from from ./output/sub-01 . Navigate to ./DATA/MMP_HCP and load in all 4 files that you see there. This is the Human Connectome's Multi-modal Parcellation of Human Cerebral Cortex [Glasser et al. 2016]. You might need to change Files of Type: to Any File (*) to be able to see them all. These files are Q1-Q6_RelatedValidation210.CorticalAreas_dil_Final_Final_Areas_Group_Colors.32k_fs_LR.dlabel.nii , Q1-Q6_RelatedValidation210.L.CorticalAreas_dil_Final_Final_Areas_Group.32k_fs_LR.border , Q1-Q6_RelatedValidation210.R.CorticalAreas_dil_Final_Final_Areas_Group.32k_fs_LR.border and MMP_areas_tangential_32k_bothHems_inflated.wb_annot Finally navigate to ./Data/100307 and load in 100307.L.flat.32k_fs_LR.surf.gii and 100307.L.sphere.32k_fs_LR.surf.gii Click on Surface View Activate the Q1-Q6_RelatedValidation210.CorticalAreas_dil_Final_Final_Areas_Group_Colors.32k_fs_LR.dlabel.nii surface by clicking it's checkbox On In the Brain Structures and Surface panel, change the surfaces in turn using the drop down box to see how the Multi Modal Parcellations are rendered on the surface. Notice how the inflated surfaces allow you to see cortical areas normally buried in the sulci. Also notice how the flat surface allows you to see both the medial and lateral surfaces at the same time. Task 2: Use MultiModal Surface Matching (MSM) to register a native mesh to the 164k fs_LR template MultiModal Surface Matching (MSM) [Robinson et al. 2014] is a spherical surface-based registration method that enhances the traditional approach to surface-based registration by allowing the use of multiple generalizable brain feature types (e.g. function, connectivity etc) to improve brain alignment. In this task however we will not be taking advantage of this enhancement and will be using MSM as any other traditional surface-based registration technique that uses only geometric features to achieve registration.. The version of MSM that is contained within the docker container is not the latest version and so takes a considerable amount of time to perform the registration for this practicum. I have thus gone ahead and created the registered spheres ahead of time which are stored in ./DATA/sub-??/MSMSulc . If you would like to repeat this part of the exercise yourself then on line 64 change the variable BYPASS=\"Y\" to BYPASS=\"N\" and run the docker container. On a 16 GB memory laptop with 8-cores this took about 50 minutes. Run the 002_register.sh script through the docker image as follows: docker run -v ${PWD}:/mnt --rm -it --entrypoint bash tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /mnt/002_register.sh In this task we are registering just the left hemisphere of a subject to the HCP's high resolution standard space 164k fs_LR. This script performs the following steps: The sulcal overlay is a CIFTI file and this has to be separated into two surface GIFTIs so that we have a measure of sulcal values for the left hemisphere and the right hemisphere The MSM algorithm then aligns the spherical surface for the subject ( sub-01.L.sphere.rot.native.surf.gii ) with the standard Left Hemisphere 164K_fsLR spherical template ( fsaverage.L_LR.spherical_std.164k_fs_LR.surf.gii ) by matching the sulcal overlay for the left hemisphere that was obtained above with the sulcal overlay template ( L.refsulc.164k_fs_LR.shape.gii ). A \"registered sphere\" ( L.sphere.reg.surf.gii ) that describes this alignment is calculated and can then be used to resample surfaces and overlays from the native space to the standard space. Using wb_command -surface-resample we resample the midthickness surface from native space to the new standard space. This new midthickness surface is called ./output/sub-01/amended.sub-01.L.midthickness.164k_fs_LR.surf.gii Using wb_command -metric-resample we resample the left cortical thickness overlay onto the new left midthickness surface in 164k_fs_LR space. Change the script so that the other subject is now processed. Change line 30 to point to the new subject ( SUB=sub-02 ) and rerun the docker command. In wb_view, select File > Close All Files and then open the two resampled surfaces for both subjects i.e. ./output/amended.sub-01/sub-01.L.midthickness.164k_fs_LR.surf.gii and ./output/amended.sub-02/sub-02.L.midthickness.164k_fs_LR.surf.gii - notice that because both surfaces are registered to the 164K template they have exactly the same number of vertices and that their vertices are in correspondence. Select Montage view and deselect the Medial checkbox. Ensure that in the Montage Selection both checkboxes are selected and that both surfaces are represented. Click on either subjects surface mesh to select a vertex and notice that a corresponding vertex is identified on the surface of the other subject. These subject meshes are in vertex correspondence as they have been registered to the same template. One dramatic way to see how features generalize across subjects is to view the sulc overlay on the sphere. Again Close all Files in your current instance of wb_view . Now open another separate instance of wb_view so that you have two copies of the program running at the same time. In both instances open the standard sphere in ./DATA/config/fsaverage.L_LR.spherical_std.164k_fs_LR.surf.gii . In one instance open the resampled sulc overlay that was created by the script at ./output/sub-01/amended.sub-01.L.sulc.164k_fs_LR.shape.gii for subject 01 and in the other instance open the resampled sulc overlay ./output/sub-02/amended.sub-02.L.sulc.164k_fs_LR.shape.gii for subject 02. Activate both overlays and click on Reset if your exploration brings your spheres out of sync to bring them back to the default start. The image below uses the Surface view to compare hemispheres however you might want to experiment with the Montage view. Notice the similarity in the sulcal maps between both subjects as you explore both spheres. Close one of the instances of wb_view when you are ready to move on to the next task. Information The sulcal overlay calculates the signed distance that any vertex needs to travel to reach its position in the fully inflated surface. Sulcal vertices will thus have positive numbers and gyral vertices will have negative numbers. This sulcal map is remarkable similar across subjects and is thus a good measure for driving surface-based registration. There are a couple of better ways than the one provided above of visualizing both subjects' overlays on the fsaverage.L_LR.spherical_std.164k_fs_LR.surf.gii surface in one instance of wb_view that have been recommended by Tim Coalson. In the first approach, instead of opening two instances of wb_view you can simply open the fsaverage.L_LR.spherical_std.164k_fs_LR.surf.gii surface and then load in both subjects' sulc overlays amended.sub-01.L.sulc.164k_fs_LR.shape.gii and amended.sub-02.L.sulc.164k_fs_LR.shape.gii . One can then enable both overlays and then disable and enable the top one to switch between viewing the two files. In a second approach, having loaded the fsaverage.L_LR.spherical_std.164k_fs_LR.surf.gii surface and the 2 sulc overlays as described above, you can close tabs until you have only 2 showing and then use \"View->Enter Tile Tabs\" with camera yoking to see two spheres with different data that automatically sync what angle they are viewed at. With this approach we do not need to use \"reset\". Task 3: Project volumetric data onto a GIFTI Surface Run the 003_projection.sh script through the docker image as follows: docker run -v ${PWD}:/mnt --rm -it --entrypoint bash tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /mnt/003_projection.sh This script uses wb_command -volume-to-surface-mapping to project the volumetric NIFTI-1 functional MRI data at ./DATA/sub-01/MNINonLinear/Results/task-rhymejudgment/task-rhymejudgment.nii.gz between the pial and white surfaces. in wb_view, select File > Close All Files and then open the midthickness surface you created in task 1 amended.sub-01.L.midthickness.32k_fs_LR.surf.gii from ./output/sub-01 and then open the projected surface overlay sub-01.L.midthickness.32k_fs_LR.func.gii which is in the same directory. Change the file from metricdynconn - sub-01.... to the sub-01.L.midthickness.32k_fs_LR.func.gii surface in the overlay toolbox. Notice that the functional MRI has been successfully projected on the surface. You can cycle through the different volumes by clicking the time index next to the Yoke column in the Overlay Toolbox. Unfortunately we are little limited in what we can do with this GIFTI functional overlay in wb_view. For example we cannot access the functional data values at individual timepoints (instead we see a long list of all the time values) and also each time point is labelled with 'ribbon constrained' rather than the unique time value in seconds. We do have access to the connectivity layer metricdynconn - sub-01.... which is created on the fly but we won't be able to see how the left cortex is connected to the right cortex or to sub-cortical structures at the same time. In the next task we will address the shortcomings of using the GIFTI overlay by creating a CIFTI file which will combine data from both surfaces and subcortical structures into one file. Task 4: Convert volumetric functional data into a CIFTI file Note The script for this task takes a little longer than the others to complete. It relies on the successful completion of Task 3 for the projection of the functional data onto the left hemisphere. Ensure that that Task 3 has been completed for the subject under study before running this task. Run the 004_ciftigen.sh script through the docker image as follows: docker run -v ${PWD}:/mnt --rm -it --entrypoint bash tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /mnt/004_ciftigen.sh This script continues the process of projecting the volume data onto the Right midthickness surface and then samples the sub-cortical data using a sub-cortical mask that covers the key grey-matter areas of the brain. The script then uses wb_command -cifti-create-dense-timeseries to combine the data on the 2 surfaces and in the sub-cortical structures into a CIFTI file. From ./output/sub-01 open the the left amended.sub-01.L.midthickness.32k_fs_LR.surf.gii and right amended.sub-01.R.midthickness.32k_fs_LR.surf.gii midthickness surfaces and the newly created CIFTI functional MRI overlay, created.den-91k.sub-01_rest.dtseries.nii . The CIFTI functional overlay provides advantages for viewing and manipulating neuroimaging data. Click on All View and change the file from dynconn - created.... to the created.den-91k.sub-01_rest.dtseries.nii layer in the overlay toolbox Notice that we can see the functional data for both hemispheres (surface-based vertices) as well as for the subcortical structures (volumetric voxels) at the same time. We can also visualize the connectivity between all voxels and vertices at the same time by changing to the dynconn - created.... layer. Task 5: Spatial Smoothing Run the 005_smoothing.sh script through the docker image as follows: docker run -v ${PWD}:/mnt --rm -it --entrypoint bash tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /mnt/005_smoothing.sh This script simply uses wb_command -cifti-smoothing to spatially smooth our CIFTI fmri file using a gaussian kernel. From ./output/sub-01 open Surfaces amended.sub-01.L.midthickness.32k_fs_LR.surf.gii and amended.sub-01.R.midthickness.32k_fs_LR.surf.gii Now also open created.den-91k.sub-01_rest.dtseries.nii and created.den-91k.sub-01_rest.smoothed.dtseries.nii In the All or Surface view activate the smoothed and non-smoothed layers. Click the top layer off and on to see the blurring effect that spatial smoothing has on the data on the cortical surface. Spatial smoothing helps to reduce the deleterious impact of noise by smoothening peaks in the signal and boosting troughs. However this also has the effect of decreasing the spatial resolution of the data. Conservative use of spatial smoothing is advised [Coalson et al. 2018]. The HCP uses a gaussian kernel of 2mm for smoothing its functional data. If region of interest approaches are used to analyze data then smoothing can be avoided altogether. In the Volume view observe the same effect for the sub-cortical voxels. Extra Time? If you have got through this quickly then here are some additional things to try: See if you can adapt the code to work for all the right hemisphere meshes and overlays. Generate all the surfaces and overlays for subject 02. Get the MSM algorithm to go through its paces in Task 2 by changing the variable BYPASS=\"Y\" to BYPASS=\"N\" Create your own script and try out different wb_commands Final words This concludes this introduction to using surface-based methods for processing data. A critical step that underpins all of the processing steps above is the creation of the initial surface meshes ( white , pial etc) and overlays ( sulc , thickness etc) by dedicated software like Freesurfer . Once these surfaces have been created then the range of processing steps described above can be accomplished using tools like wb_command , freesurfer modules, custom matlab or python scripts etc. To gain more experience using wb_command then material from the 2019 wb_command course might be useful. Online help on individual workbench commands is available here . More complex common procedures that require a sequence of different wb_command calls to implement can be called through the wb_shortcuts script. Missing from this practicum is any demonstration of how to perform group analysis using surface-based formats. An additional practicum is in development which will attempt to cover basic group analysis on the surface using tools like film_gls and PALM which are both part of the FSL ecosystem and which have been used for the HCP's surface-based task analysis . Development is occurring in this area quite rapidly and user-friendly tools that facilitate statistical analysis on the surface should be available soon. Please provide any corrections and suggestions by email or as issues on GitHub Acknowledgements Thanks to Dianne Patterson for sharing her comprehensive notes on wb_command which were very helpful and for providing helpful feedback on the practicum. Thanks also to Tim Coalson for directing me to the HCP's 2019 wb_command course which provided inspiration. Huge thanks to Tim Coalson for very generously spending time to clarify misconceptions I had about Surface-Based methods and for several critical corrections on use of terminology that went a long way to improving the content here. I am much indebted to him for his attention to detail and his clear, comprehensive and extremely helpful feedback. References Coalson, T. S., Van Essen, D. C., & Glasser, M. F. (2018). The impact of traditional neuroimaging methods on the spatial localization of cortical areas. Proceedings of the National Academy of Sciences, 115(27), E6356-E6365. Dickie, E. W., Anticevic, A., Smith, D. E., Coalson, T. S., Manogaran, M., Calarco, N., ... & Voineskos, A. N. (2019). ciftify: A framework for surface-based analysis of legacy MR acquisitions. Neuroimage, 197, 818-826. Esteban, O., Ciric, R., Finc, K., Blair, R. W., Markiewicz, C. J., Moodie, C. A., ... & Ye, Z. (2020). Analysis of task-based functional MRI data preprocessed with fMRIPrep. Nature Protocols, 1-17. Glasser, M. F., Sotiropoulos, S. N., Wilson, J. A., Coalson, T. S., Fischl, B., Andersson, J. L., ... & Van Essen, D. C. (2013). The minimal preprocessing pipelines for the Human Connectome Project. Neuroimage, 80, 105-124. Glasser, M. F., Coalson, T. S., Robinson, E. C., Hacker, C. D., Harwell, J., Yacoub, E., ... & Smith, S. M. (2016). A multi-modal parcellation of human cerebral cortex. Nature, 536(7615), 171-178. Robinson, E. C., Jbabdi, S., Glasser, M. F., Andersson, J., Burgess, G. C., Harms, M. P., ... & Jenkinson, M. (2014). MSM: a new flexible framework for Multimodal Surface Matching . Neuroimage, 100, 414-426. Xue, G., & Poldrack, R. A. (2007). The neural substrates of visual perceptual learning of words: implications for the visual word form area hypothesis. Journal of cognitive neuroscience, 19(10), 1643-1655.","title":"practicum 9.1"},{"location":"surfanalysis/prac91/#practicum-91-surface-based-analysis","text":"","title":"Practicum 9.1 -  Surface-Based Analysis"},{"location":"surfanalysis/prac91/#synopsis","text":"Surface-based methods provide several advantages over volume-based methods for analysing data on the cerebral cortex [Dickie et al. 2019, Coalson et al. 2018, Glasser et al. 2013]. Fundamentally, they seem to provide a more neurobiologically valid representation of the way that function is mapped along the surface of the cortex as a 2D metric [Glasser et al. 2013] . This in turn affords other downstream benefits that make surface-based approaches attractive for studying the cortex. These benefits include the facilitation of more computationally tractable algorithms for aligning cortical areas across many subjects using brain measures that lead to better alignment of functional areas [Coalson et al. 2018]. The HCP for example uses brain features like myelination and resting state fMRI to improve functional correspondence between subjects which would not be guaranteed by the restricted geometric-based measures used by Volume-Based methods. Another benefit of surface-based methods is the encouragement of constrained spatial smoothing methods that reduce the corruption of functional signal from non-related parts of the brain [Dickie et al. 2019]. These two benefits ensure that studies can be conducted accurately at high resolution across a large number of subjects and help improve the statistical power of surface based studies. Surface-based methods also allow for much better overall intuitive visualization of cortical patterns of structure and function when compared to volume-based visualization on a slice-by-slice basis. In addition, surface-based models of the cortex by nature lead to more compact and efficient representation. For example, to achieve 2mm resolution of the cortex, the Human Connectome Project uses a CIFTI file with 91282 vertices and voxels to represent gray matter in the brain. By contrast, an MNI 2mm mask requires approximately 130,000 voxels to cover the gray matter region in volumetric analysis. In addition to reduced file sizes the use of surface-based data formats easilly allows multi-modal data to be quickly aligned and matched within and across subjects greatly improving the speed and ease of analysis [Dickie et al. 2019].","title":"Synopsis"},{"location":"surfanalysis/prac91/#objectives","text":"This practicum will introduce you to some of the steps required for preparing MRI neuroimaging data for group surface-based statistical analysis. These steps have been copied from the Human Connectome Project's Pipelines [Glasser et al. 2013] which have been recreated by Dickie et al. 2019 for non-HCP (aka legacy) data. The scripts for this practicum are described below: 001_inflate.sh : Use wb_command to create midthickness surface and inflated surfaces. 002_register.sh : Use msm and wb_command to register 2 subject meshes to a standard surface template 003_projection.sh : Use wb_command to map an fmri volume onto a GIFTI surface mesh 004_ciftigen.sh : Use wb_command to map an fmri volume into a CIFTI file 005_smoothing.sh : Use wb_command to perform surface-based spatial smoothing on the left and right hemispheres, as well as on the subcortical volume. Note Please appreciate that the processing tasks you will undertake in this practicum are provided for demonstration purposes only and to shed some light on how these surface processing steps are roughly accomplished. For your research you will want to use one of the robust, established pipelines that not only automate the steps above but utilize the most up-to-date and accurate methods for implementing these steps. The one step that is usually left to the user to do by these pipelines is spatial smoothing . However it is advised to minimize the use of spatial smoohing , even surface-based smoothing, in order to preserve as much spatial detail as possible in the results [Coalson et al. 2018]. FMRIPrep currently performs robust pre-processing on fmri data and can output this out as a surface-based CIFTI .dtseries.nii file for surface based analysis. If you have HCP-style data then you may be able to adapt the HCP Pipelines to work with your acquisitions. Alternatively if you have data that does not meet the HCP's recommended acquisition guidelines then the Ciftify pipeline can be used to convert your data to cifti format and map it to a similar file structure as the HCP's processing pipeline outputs. And of course Freesurfer has many different tools for surface-based analysis. Many of the steps mentioned above can be accomplished with equivalent freesurfer commands. Most notable of course is that all of the above pipelines depend on freesurfer for the creation of the initial surface mesh models. In addition to these there are several other tools and methods that can provide a good starting point for developing robust surface-based processing pipelines.","title":"Objectives"},{"location":"surfanalysis/prac91/#downloadsinstallation","text":"This practicum requires the following materials/folder setup for successful completion: A copy of the HCP's Connectome workbench for access to wb_view . You will also need the Ciftify docker image which you can obtain as follows docker pull tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 . This will allow us to run our scripts that call wb_command in a windows environment. It will also give us access to msm if you decide at a later point to fully run the registration step. Note We will be running the scripts using the docker container as shown below. For each script a command line will be provided which you will be able to copy and paste into your terminal window. Try to avoid extracting the zip folder into a root directory that has spaces in it to avoid problems with binding the current directory $PWD. If you do have to use a root directory with spaces in it then you can use double quotes around $PWD to solve this as follows -v \"${PWD}\":/mnt. docker run -v ${PWD}:/mnt --rm -it --entrypoint bash tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /mnt/ SCRIPTNAME The data and scripts for this practicum can be downloaded from the OSF site under the 09-Surface-Based-Analysis folder in a zip file practicum91.zip . Download this zip file to your local computer and extract. Your folder structure should look like this after extraction. practicum91 \u251c\u2500\u2500 001_inflate.sh \u251c\u2500\u2500 002_register.sh \u251c\u2500\u2500 003_projection.sh \u251c\u2500\u2500 004_ciftigen.sh \u251c\u2500\u2500 005_smoothing.sh \u2514\u2500\u2500 DATA \u251c\u2500\u2500 100307 \u251c\u2500\u2500 config \u251c\u2500\u2500 MMP_HCP \u251c\u2500\u2500 sub-01 \u2514\u2500\u2500 sub-02","title":"Downloads/Installation"},{"location":"surfanalysis/prac91/#provenance","text":"","title":"Provenance"},{"location":"surfanalysis/prac91/#scripts","text":"The scripts for this practicum have been written based on an inspection of the HCP pipelines and the Ciftify pipelines . Only the essential aspects of the processing steps used in these pipelines have been provided for demonstration purposes and so these scripts are suboptimal for actual research purposes and additionally have not been tested thoroughly.","title":"Scripts"},{"location":"surfanalysis/prac91/#data","text":"We will be using a subset of the Rhyme Judgment data (subjects sub-01 and sub-02 ) acquired by [Xue and Poldrack 2007]. This data has also been used in an FMRIPrep exemplar pipeline for task-based analysis [Esteban et al. 2020]. The data has been processed using fmriprep, freesurfer and ciftify as described further below. We will also use flat and spherical surfaces from subject 100307 which is included as part of the Human Connectome Project's Young Adult (YA) 1200 Subject Release . The HCP places data use terms on their data. To use HCP data please sign on to the HCP website and register for free. The folder MMP_HCP contains a subset of the files released for the Multi-modal parcellation of Human Cerebral Cortex . This atlas is a comprehensive delineation of about 180 cerebral cortex areas per hemisphere obtained using the HCP's advanced analysis approach [Glasser et al. 2013, Glasser et al. 2016]. Miscellaneous HCP configuration files (atlas and registration templates, MSM Sulc registration config file etc.) are provided in config . These have been copied from the HCP Pipelines","title":"Data"},{"location":"surfanalysis/prac91/#initial-processing","text":"The data was processed initially through FMRIPrep to create aligned fMRI volumes in anatomical space. FMRIprep also uses Freesurfer to generate the initial surface meshes for each subject. The fmriprep processed data was then used as the input to Ciftify to create the final dataset that you have available to you. Several of the Ciftify outputs that are not required for this practicum have been deleted to keep the download manageable.","title":"Initial processing"},{"location":"surfanalysis/prac91/#task-1-create-midthickness-inflated-and-very-inflated-surfaces","text":"Open a terminal and navigate to the practicum91 folder. Run the 001_inflate.sh script through the docker image as follows: docker run -v ${PWD}:/mnt --rm -it --entrypoint bash tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /mnt/001_inflate.sh The script creates the midthickness surface amended.sub-01.L.midthickness.32k_fs_LR.surf.gii in the directory practicum91/output/sub-01 . This surface lies between the pial and the white surface and is created using wb_command -surface-average . This surface is useful for projecting functional data from the volume onto the cortex. The script also creates an inflated and a very_inflated surface in the same directory using wb_command -surface-generate-inflated . These surfaces allow functional areas to be better visualized in the cortex. Note The script generates the inflated surfaces from the midthickness surface. This is similar to how it's done in the HCP pipelines but not how it's done in Freesurfer which instead uses the white surface to create the inflated surfaces. If you want to peform this on subject 02 as well then just change the subject variable on line 27 in 001_inflate.sh using a simple text editor, as follows SUB=sub-02 and rerun the docker command. Open wb_view and navigate to ./DATA/sub-01/MNINonLinear/ and load in T1w.nii.gz . Now navigate up one folder to fsaverage_LR32k and load in the two surfaces sub-01.L.pial.32k_fs_LR.surf.gii and sub-01.L.white.32k_fs_LR.surf.gii Now load in the midthickness surface you created amended.sub-01.L.midthickness.32k_fs_LR.surf.gii from ./output/sub-01 Click on the volume view and select the Vol/Surf Tab . Assign each drop down in the File column to one of the three different overlays and assign each a different color. Notice that your midthickness surface is indeed between the pial and the white surface. You may have to zoom in to see this more clearly. Now we will see how the inflated and very_inflated surfaces better help us visualize cortical areas. We will also look at the flat surface and a spherical surface that are provided by the HCP's pipelines. In wb_view now load in the inflated amended.sub-01.L.inflated.32k_fs_LR.surf.gii and very_inflated surfaces amended.sub-01.L.very_inflated.32k_fs_LR.surf.gii from from ./output/sub-01 . Navigate to ./DATA/MMP_HCP and load in all 4 files that you see there. This is the Human Connectome's Multi-modal Parcellation of Human Cerebral Cortex [Glasser et al. 2016]. You might need to change Files of Type: to Any File (*) to be able to see them all. These files are Q1-Q6_RelatedValidation210.CorticalAreas_dil_Final_Final_Areas_Group_Colors.32k_fs_LR.dlabel.nii , Q1-Q6_RelatedValidation210.L.CorticalAreas_dil_Final_Final_Areas_Group.32k_fs_LR.border , Q1-Q6_RelatedValidation210.R.CorticalAreas_dil_Final_Final_Areas_Group.32k_fs_LR.border and MMP_areas_tangential_32k_bothHems_inflated.wb_annot Finally navigate to ./Data/100307 and load in 100307.L.flat.32k_fs_LR.surf.gii and 100307.L.sphere.32k_fs_LR.surf.gii Click on Surface View Activate the Q1-Q6_RelatedValidation210.CorticalAreas_dil_Final_Final_Areas_Group_Colors.32k_fs_LR.dlabel.nii surface by clicking it's checkbox On In the Brain Structures and Surface panel, change the surfaces in turn using the drop down box to see how the Multi Modal Parcellations are rendered on the surface. Notice how the inflated surfaces allow you to see cortical areas normally buried in the sulci. Also notice how the flat surface allows you to see both the medial and lateral surfaces at the same time.","title":"Task 1: Create midthickness, inflated and very-inflated surfaces"},{"location":"surfanalysis/prac91/#task-2-use-multimodal-surface-matching-msm-to-register-a-native-mesh-to-the-164k-fs_lr-template","text":"MultiModal Surface Matching (MSM) [Robinson et al. 2014] is a spherical surface-based registration method that enhances the traditional approach to surface-based registration by allowing the use of multiple generalizable brain feature types (e.g. function, connectivity etc) to improve brain alignment. In this task however we will not be taking advantage of this enhancement and will be using MSM as any other traditional surface-based registration technique that uses only geometric features to achieve registration.. The version of MSM that is contained within the docker container is not the latest version and so takes a considerable amount of time to perform the registration for this practicum. I have thus gone ahead and created the registered spheres ahead of time which are stored in ./DATA/sub-??/MSMSulc . If you would like to repeat this part of the exercise yourself then on line 64 change the variable BYPASS=\"Y\" to BYPASS=\"N\" and run the docker container. On a 16 GB memory laptop with 8-cores this took about 50 minutes. Run the 002_register.sh script through the docker image as follows: docker run -v ${PWD}:/mnt --rm -it --entrypoint bash tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /mnt/002_register.sh In this task we are registering just the left hemisphere of a subject to the HCP's high resolution standard space 164k fs_LR. This script performs the following steps: The sulcal overlay is a CIFTI file and this has to be separated into two surface GIFTIs so that we have a measure of sulcal values for the left hemisphere and the right hemisphere The MSM algorithm then aligns the spherical surface for the subject ( sub-01.L.sphere.rot.native.surf.gii ) with the standard Left Hemisphere 164K_fsLR spherical template ( fsaverage.L_LR.spherical_std.164k_fs_LR.surf.gii ) by matching the sulcal overlay for the left hemisphere that was obtained above with the sulcal overlay template ( L.refsulc.164k_fs_LR.shape.gii ). A \"registered sphere\" ( L.sphere.reg.surf.gii ) that describes this alignment is calculated and can then be used to resample surfaces and overlays from the native space to the standard space. Using wb_command -surface-resample we resample the midthickness surface from native space to the new standard space. This new midthickness surface is called ./output/sub-01/amended.sub-01.L.midthickness.164k_fs_LR.surf.gii Using wb_command -metric-resample we resample the left cortical thickness overlay onto the new left midthickness surface in 164k_fs_LR space. Change the script so that the other subject is now processed. Change line 30 to point to the new subject ( SUB=sub-02 ) and rerun the docker command. In wb_view, select File > Close All Files and then open the two resampled surfaces for both subjects i.e. ./output/amended.sub-01/sub-01.L.midthickness.164k_fs_LR.surf.gii and ./output/amended.sub-02/sub-02.L.midthickness.164k_fs_LR.surf.gii - notice that because both surfaces are registered to the 164K template they have exactly the same number of vertices and that their vertices are in correspondence. Select Montage view and deselect the Medial checkbox. Ensure that in the Montage Selection both checkboxes are selected and that both surfaces are represented. Click on either subjects surface mesh to select a vertex and notice that a corresponding vertex is identified on the surface of the other subject. These subject meshes are in vertex correspondence as they have been registered to the same template. One dramatic way to see how features generalize across subjects is to view the sulc overlay on the sphere. Again Close all Files in your current instance of wb_view . Now open another separate instance of wb_view so that you have two copies of the program running at the same time. In both instances open the standard sphere in ./DATA/config/fsaverage.L_LR.spherical_std.164k_fs_LR.surf.gii . In one instance open the resampled sulc overlay that was created by the script at ./output/sub-01/amended.sub-01.L.sulc.164k_fs_LR.shape.gii for subject 01 and in the other instance open the resampled sulc overlay ./output/sub-02/amended.sub-02.L.sulc.164k_fs_LR.shape.gii for subject 02. Activate both overlays and click on Reset if your exploration brings your spheres out of sync to bring them back to the default start. The image below uses the Surface view to compare hemispheres however you might want to experiment with the Montage view. Notice the similarity in the sulcal maps between both subjects as you explore both spheres. Close one of the instances of wb_view when you are ready to move on to the next task. Information The sulcal overlay calculates the signed distance that any vertex needs to travel to reach its position in the fully inflated surface. Sulcal vertices will thus have positive numbers and gyral vertices will have negative numbers. This sulcal map is remarkable similar across subjects and is thus a good measure for driving surface-based registration. There are a couple of better ways than the one provided above of visualizing both subjects' overlays on the fsaverage.L_LR.spherical_std.164k_fs_LR.surf.gii surface in one instance of wb_view that have been recommended by Tim Coalson. In the first approach, instead of opening two instances of wb_view you can simply open the fsaverage.L_LR.spherical_std.164k_fs_LR.surf.gii surface and then load in both subjects' sulc overlays amended.sub-01.L.sulc.164k_fs_LR.shape.gii and amended.sub-02.L.sulc.164k_fs_LR.shape.gii . One can then enable both overlays and then disable and enable the top one to switch between viewing the two files. In a second approach, having loaded the fsaverage.L_LR.spherical_std.164k_fs_LR.surf.gii surface and the 2 sulc overlays as described above, you can close tabs until you have only 2 showing and then use \"View->Enter Tile Tabs\" with camera yoking to see two spheres with different data that automatically sync what angle they are viewed at. With this approach we do not need to use \"reset\".","title":"Task 2: Use MultiModal Surface Matching (MSM) to register a native mesh to the 164k fs_LR template"},{"location":"surfanalysis/prac91/#task-3-project-volumetric-data-onto-a-gifti-surface","text":"Run the 003_projection.sh script through the docker image as follows: docker run -v ${PWD}:/mnt --rm -it --entrypoint bash tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /mnt/003_projection.sh This script uses wb_command -volume-to-surface-mapping to project the volumetric NIFTI-1 functional MRI data at ./DATA/sub-01/MNINonLinear/Results/task-rhymejudgment/task-rhymejudgment.nii.gz between the pial and white surfaces. in wb_view, select File > Close All Files and then open the midthickness surface you created in task 1 amended.sub-01.L.midthickness.32k_fs_LR.surf.gii from ./output/sub-01 and then open the projected surface overlay sub-01.L.midthickness.32k_fs_LR.func.gii which is in the same directory. Change the file from metricdynconn - sub-01.... to the sub-01.L.midthickness.32k_fs_LR.func.gii surface in the overlay toolbox. Notice that the functional MRI has been successfully projected on the surface. You can cycle through the different volumes by clicking the time index next to the Yoke column in the Overlay Toolbox. Unfortunately we are little limited in what we can do with this GIFTI functional overlay in wb_view. For example we cannot access the functional data values at individual timepoints (instead we see a long list of all the time values) and also each time point is labelled with 'ribbon constrained' rather than the unique time value in seconds. We do have access to the connectivity layer metricdynconn - sub-01.... which is created on the fly but we won't be able to see how the left cortex is connected to the right cortex or to sub-cortical structures at the same time. In the next task we will address the shortcomings of using the GIFTI overlay by creating a CIFTI file which will combine data from both surfaces and subcortical structures into one file.","title":"Task 3: Project volumetric data onto a GIFTI Surface"},{"location":"surfanalysis/prac91/#task-4-convert-volumetric-functional-data-into-a-cifti-file","text":"Note The script for this task takes a little longer than the others to complete. It relies on the successful completion of Task 3 for the projection of the functional data onto the left hemisphere. Ensure that that Task 3 has been completed for the subject under study before running this task. Run the 004_ciftigen.sh script through the docker image as follows: docker run -v ${PWD}:/mnt --rm -it --entrypoint bash tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /mnt/004_ciftigen.sh This script continues the process of projecting the volume data onto the Right midthickness surface and then samples the sub-cortical data using a sub-cortical mask that covers the key grey-matter areas of the brain. The script then uses wb_command -cifti-create-dense-timeseries to combine the data on the 2 surfaces and in the sub-cortical structures into a CIFTI file. From ./output/sub-01 open the the left amended.sub-01.L.midthickness.32k_fs_LR.surf.gii and right amended.sub-01.R.midthickness.32k_fs_LR.surf.gii midthickness surfaces and the newly created CIFTI functional MRI overlay, created.den-91k.sub-01_rest.dtseries.nii . The CIFTI functional overlay provides advantages for viewing and manipulating neuroimaging data. Click on All View and change the file from dynconn - created.... to the created.den-91k.sub-01_rest.dtseries.nii layer in the overlay toolbox Notice that we can see the functional data for both hemispheres (surface-based vertices) as well as for the subcortical structures (volumetric voxels) at the same time. We can also visualize the connectivity between all voxels and vertices at the same time by changing to the dynconn - created.... layer.","title":"Task 4: Convert volumetric functional data into a CIFTI file"},{"location":"surfanalysis/prac91/#task-5-spatial-smoothing","text":"Run the 005_smoothing.sh script through the docker image as follows: docker run -v ${PWD}:/mnt --rm -it --entrypoint bash tigrlab/fmriprep_ciftify:v1.3.2-2.3.3 /mnt/005_smoothing.sh This script simply uses wb_command -cifti-smoothing to spatially smooth our CIFTI fmri file using a gaussian kernel. From ./output/sub-01 open Surfaces amended.sub-01.L.midthickness.32k_fs_LR.surf.gii and amended.sub-01.R.midthickness.32k_fs_LR.surf.gii Now also open created.den-91k.sub-01_rest.dtseries.nii and created.den-91k.sub-01_rest.smoothed.dtseries.nii In the All or Surface view activate the smoothed and non-smoothed layers. Click the top layer off and on to see the blurring effect that spatial smoothing has on the data on the cortical surface. Spatial smoothing helps to reduce the deleterious impact of noise by smoothening peaks in the signal and boosting troughs. However this also has the effect of decreasing the spatial resolution of the data. Conservative use of spatial smoothing is advised [Coalson et al. 2018]. The HCP uses a gaussian kernel of 2mm for smoothing its functional data. If region of interest approaches are used to analyze data then smoothing can be avoided altogether. In the Volume view observe the same effect for the sub-cortical voxels.","title":"Task 5: Spatial Smoothing"},{"location":"surfanalysis/prac91/#extra-time","text":"If you have got through this quickly then here are some additional things to try: See if you can adapt the code to work for all the right hemisphere meshes and overlays. Generate all the surfaces and overlays for subject 02. Get the MSM algorithm to go through its paces in Task 2 by changing the variable BYPASS=\"Y\" to BYPASS=\"N\" Create your own script and try out different wb_commands","title":"Extra Time?"},{"location":"surfanalysis/prac91/#final-words","text":"This concludes this introduction to using surface-based methods for processing data. A critical step that underpins all of the processing steps above is the creation of the initial surface meshes ( white , pial etc) and overlays ( sulc , thickness etc) by dedicated software like Freesurfer . Once these surfaces have been created then the range of processing steps described above can be accomplished using tools like wb_command , freesurfer modules, custom matlab or python scripts etc. To gain more experience using wb_command then material from the 2019 wb_command course might be useful. Online help on individual workbench commands is available here . More complex common procedures that require a sequence of different wb_command calls to implement can be called through the wb_shortcuts script. Missing from this practicum is any demonstration of how to perform group analysis using surface-based formats. An additional practicum is in development which will attempt to cover basic group analysis on the surface using tools like film_gls and PALM which are both part of the FSL ecosystem and which have been used for the HCP's surface-based task analysis . Development is occurring in this area quite rapidly and user-friendly tools that facilitate statistical analysis on the surface should be available soon. Please provide any corrections and suggestions by email or as issues on GitHub","title":"Final words"},{"location":"surfanalysis/prac91/#acknowledgements","text":"Thanks to Dianne Patterson for sharing her comprehensive notes on wb_command which were very helpful and for providing helpful feedback on the practicum. Thanks also to Tim Coalson for directing me to the HCP's 2019 wb_command course which provided inspiration. Huge thanks to Tim Coalson for very generously spending time to clarify misconceptions I had about Surface-Based methods and for several critical corrections on use of terminology that went a long way to improving the content here. I am much indebted to him for his attention to detail and his clear, comprehensive and extremely helpful feedback.","title":"Acknowledgements"},{"location":"surfanalysis/prac91/#references","text":"Coalson, T. S., Van Essen, D. C., & Glasser, M. F. (2018). The impact of traditional neuroimaging methods on the spatial localization of cortical areas. Proceedings of the National Academy of Sciences, 115(27), E6356-E6365. Dickie, E. W., Anticevic, A., Smith, D. E., Coalson, T. S., Manogaran, M., Calarco, N., ... & Voineskos, A. N. (2019). ciftify: A framework for surface-based analysis of legacy MR acquisitions. Neuroimage, 197, 818-826. Esteban, O., Ciric, R., Finc, K., Blair, R. W., Markiewicz, C. J., Moodie, C. A., ... & Ye, Z. (2020). Analysis of task-based functional MRI data preprocessed with fMRIPrep. Nature Protocols, 1-17. Glasser, M. F., Sotiropoulos, S. N., Wilson, J. A., Coalson, T. S., Fischl, B., Andersson, J. L., ... & Van Essen, D. C. (2013). The minimal preprocessing pipelines for the Human Connectome Project. Neuroimage, 80, 105-124. Glasser, M. F., Coalson, T. S., Robinson, E. C., Hacker, C. D., Harwell, J., Yacoub, E., ... & Smith, S. M. (2016). A multi-modal parcellation of human cerebral cortex. Nature, 536(7615), 171-178. Robinson, E. C., Jbabdi, S., Glasser, M. F., Andersson, J., Burgess, G. C., Harms, M. P., ... & Jenkinson, M. (2014). MSM: a new flexible framework for Multimodal Surface Matching . Neuroimage, 100, 414-426. Xue, G., & Poldrack, R. A. (2007). The neural substrates of visual perceptual learning of words: implications for the visual word form area hypothesis. Journal of cognitive neuroscience, 19(10), 1643-1655.","title":"References"},{"location":"surfdata/cifti-tools/","text":"Cifti Tools A list of CIFTI-compliant tools is maintained by the HCP team. Selective comments on implementation details of these tools are supplied further below. cifti-matlab beta release v2.0.0 Comments on the new Beta-release version 2.0.0 of the cifti-matlab release are provided below from an email sent by Tim Coalson to the Google Groups \"HCP-Users\". Information As has been mentioned occasionally, we have been working on a new matlab library for reading CIFTI files, that exposes all the internal information in the CIFTI XML, and does not rely on external tools for the common case of working with CIFTI-2 files. We think we have reached near-final function naming and argument conventions for the current functions, and that it has been tested enough for initial testing/use by the neuroscience community, so we are now releasing its current state as a beta version, here: https://github.com/Washington-University/cifti-matlab/releases/tag/v2.0.0-beta.1 A primary design feature is that it can function as a drop-in replacement for ciftiopen/ciftisave/ciftisavereset without editing any existing code. A benefit from even this simple replacement is that it does less file IO, since the original ciftiopen called wb_command to convert the entire CIFTI file to a different format, and as such, we plan to switch the HCP pipelines to use the new library in the near future. The new library also provides some helper functions, such as extracting the surface or volume data or replacing it with new data (organized like an ordinary volume or metric file - basically, wb_command -cifti-separate and -cifti-replace-structure, but without leaving matlab), and there will likely be more added in the future. The ft_read_cifti and related code (cifti-matlab v1) that was previously available from this github repository is currently available in the \"ft_cifti\" folder, or at the \"v1-final\" tag.","title":"Cifti Tools"},{"location":"surfdata/cifti-tools/#cifti-tools","text":"A list of CIFTI-compliant tools is maintained by the HCP team. Selective comments on implementation details of these tools are supplied further below.","title":"Cifti Tools"},{"location":"surfdata/cifti-tools/#cifti-matlab-beta-release-v200","text":"Comments on the new Beta-release version 2.0.0 of the cifti-matlab release are provided below from an email sent by Tim Coalson to the Google Groups \"HCP-Users\". Information As has been mentioned occasionally, we have been working on a new matlab library for reading CIFTI files, that exposes all the internal information in the CIFTI XML, and does not rely on external tools for the common case of working with CIFTI-2 files. We think we have reached near-final function naming and argument conventions for the current functions, and that it has been tested enough for initial testing/use by the neuroscience community, so we are now releasing its current state as a beta version, here: https://github.com/Washington-University/cifti-matlab/releases/tag/v2.0.0-beta.1 A primary design feature is that it can function as a drop-in replacement for ciftiopen/ciftisave/ciftisavereset without editing any existing code. A benefit from even this simple replacement is that it does less file IO, since the original ciftiopen called wb_command to convert the entire CIFTI file to a different format, and as such, we plan to switch the HCP pipelines to use the new library in the near future. The new library also provides some helper functions, such as extracting the surface or volume data or replacing it with new data (organized like an ordinary volume or metric file - basically, wb_command -cifti-separate and -cifti-replace-structure, but without leaving matlab), and there will likely be more added in the future. The ft_read_cifti and related code (cifti-matlab v1) that was previously available from this github repository is currently available in the \"ft_cifti\" folder, or at the \"v1-final\" tag.","title":"cifti-matlab beta release v2.0.0"},{"location":"surfdata/formatref/","text":"Neuroimaging Data Formats DICOM Dicom information stored as tags. The Patient Coordinate System (PCS) is usually defined in LPS+ format but this could be different depending on the [Anatomical Orientation Type Field (0010,2210)]https://nipy.org/nibabel/dicom/dicom_orientation.html. The pixel data (7FE0, 0010) is mapped into a 3D matrix in voxel space using additional information from the fields Image Position (0020,0032) and Image Orientation (0020,0037). This data is mapped for each image plane in row-major order from left to right, top to bottom. Nifti-1 References Brainder blog has a nice detailed descrption of Nifti-2 and Nifti-1 Gifti Gifti Library This library is matlab based and in addition to standard Gifti files with extension .gii can read in and export to a range of surface file formats like freesurfer .pial , .sphere , .white , .surf , .curv etc, as well as matlab .mat files, .vtk files and several others. This library is integrated into SPM . You can download a release version gifti-zip from here In linux command shell you can do: cd library wget https://github.com/gllmflndn/gifti/archive/master.zip unzip master.zip rm master.zip mv gifti-master gifti-release For development updates, usage and to get latest code go to github page. cd library git clone https://github.com/gllmflndn/gifti.git mv gifti gifti-latest Open Gifti File example % add release library to path addpath /home/chidi/repos/CFN/SurfaceFormat/library/gifti-release % read and view the GIFTI surface mesh surfacemesh='./exampleData/BV_GIFTI/GzipBase64/sujet01_Lwhite.surf.gii' gmesh = gifti(surfacemesh); figure; plot(gmesh) %read the curvature and display on mesh curvature='./exampleData/BV_GIFTI/GzipBase64/sujet01_Lwhite.shape.gii'; gcurv = gifti(curvature); % plot mesh with curvature figure; plot(gmesh,gcurv) Create Gifti File example % add release library to path addpath /home/chidi/repos/CFN/SurfaceFormat/library/gifti-release load mri D = squeeze(D); Ds = smooth3(D); g = gifti(isosurface(Ds,5)) h = plot(g); daspect([1,1,.4]); view(45,30); axis tight lightangle(45,30); set(h,'SpecularColorReflectance',0,'SpecularExponent',50) save(g,'mri.surf.gii','Base64Binary'); gmri = gifti('mri.surf.gii'); figure; plot(gmri) Viewing the XML header The gifti file itself is basically a large xml file that describes th metadata and data contained for the surface. In ASCII format then it is simple enough to just open it in a text editor and peruse the contents. In some cases the data is encoded as Base64 binary and compressed which the gifti library can manage. In order to see the xml header for files that are not ascii then you can simple open the gifti and then resave it as an ascii gifti as follows surfacemesh='./exampleData/BV_GIFTI/GzipBase64/sujet01_Lwhite.surf.gii' gmesh = gifti(surfacemesh); save(gmesh,'asAscii.gii','ASCII'); Nifti-2 References Brainder blog has a nice detailed descrption of Nifti-2 and Nifti-1 Cifti Data An assortment of CIFTI and GIFTI files are available from OSF: If you have the python client osfclient installed then use: osf -p hetgq fetch /Data/HCP . Python Possible free jupyter option is https://colab.research.google.com Matlab There are libraries available in matlab for working with CIFTI. This FAQ from the Human Connectome Project provides 3 approaches for loading CIFTI files. For HCP Approach 1 : For HCP MEG data The FieldTrip toolbox provides a matlab library for working with CIFTI files. It is for use with HCP MEG Data. It is not advised for use with MRI data because it pads the CIFTI matrix with NaN values however it is a completely independent library. The field trip toolbox functions are prefixed by ft_ as in ft_read_cifti Approach 2 : For HCP MRI data From the HCP FAQ download just the following 3 matlab functions ciftiopen.m , ciftisave.m and ciftisavereset.m and place in your path. You will need need the gifti matlab library as well. You can download a release version gifti-zip from here . Unzip to a folder and in matlab add to your path as addpath /path/to/gifti We will also need access to wb_command Approach 3: Alpha Testing for HCP MRI Data This library is in alpha testing phase and is based on the Field Trip fieldbox. It should resolve the issues with Approach 1 but it is still strictly in development and so changes are possible. The field trip library that it is based on is also included in subfolder ft_cifti For this exploration we will proceed with Approach 3 git clone https://github.com/Washington-University/cifti-matlab.git Container Singularity and Docker containers with the HCP Workbench are available for exploring CIFTI data. The singularity image can be run as follows: singularity run --nv -B $PWD:/mnt workbench.sif --homedir /mnt wb_view The docker image can be run as docker run --rm --user=developer -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v $HOME/.Xauthority:/home/developer/.Xauthority -it --net=host --pid=host --ipc=host aacazxnat/workbench:0.1 wb_view Problems with opening wb_view using Docker on gpu enabled computer You will need to add Nvidia-Docker to your system. See Github for instructions for installation which are replicated below: distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit sudo systemctl restart docker Test nvidia-smi with the latest official CUDA image docker run --gpus all nvidia/cuda:10.2-base nvidia-smi Now try the gpu image: docker run --gpus all --rm --user=developer -e DISPLAY=$DISPLAY -v /usr/lib/nvidia:/usr/lib/nvidia -v /tmp/.X11-unix:/tmp/.X11-unix -v $HOME:/mnt -v $HOME/.Xauthority:/home/developer/.Xauthority -it --net=host --pid=host --device /dev/dri --ipc=host --privileged aacazxnat/workbenchgpu_runtime:0.1 --homedir /mnt wb_view","title":"Neuroimaging Data Formats"},{"location":"surfdata/formatref/#neuroimaging-data-formats","text":"","title":"Neuroimaging Data Formats"},{"location":"surfdata/formatref/#dicom","text":"Dicom information stored as tags. The Patient Coordinate System (PCS) is usually defined in LPS+ format but this could be different depending on the [Anatomical Orientation Type Field (0010,2210)]https://nipy.org/nibabel/dicom/dicom_orientation.html. The pixel data (7FE0, 0010) is mapped into a 3D matrix in voxel space using additional information from the fields Image Position (0020,0032) and Image Orientation (0020,0037). This data is mapped for each image plane in row-major order from left to right, top to bottom.","title":"DICOM"},{"location":"surfdata/formatref/#nifti-1","text":"","title":"Nifti-1"},{"location":"surfdata/formatref/#references","text":"Brainder blog has a nice detailed descrption of Nifti-2 and Nifti-1","title":"References"},{"location":"surfdata/formatref/#gifti","text":"","title":"Gifti"},{"location":"surfdata/formatref/#gifti-library","text":"This library is matlab based and in addition to standard Gifti files with extension .gii can read in and export to a range of surface file formats like freesurfer .pial , .sphere , .white , .surf , .curv etc, as well as matlab .mat files, .vtk files and several others. This library is integrated into SPM . You can download a release version gifti-zip from here In linux command shell you can do: cd library wget https://github.com/gllmflndn/gifti/archive/master.zip unzip master.zip rm master.zip mv gifti-master gifti-release For development updates, usage and to get latest code go to github page. cd library git clone https://github.com/gllmflndn/gifti.git mv gifti gifti-latest","title":"Gifti Library"},{"location":"surfdata/formatref/#open-gifti-file-example","text":"% add release library to path addpath /home/chidi/repos/CFN/SurfaceFormat/library/gifti-release % read and view the GIFTI surface mesh surfacemesh='./exampleData/BV_GIFTI/GzipBase64/sujet01_Lwhite.surf.gii' gmesh = gifti(surfacemesh); figure; plot(gmesh) %read the curvature and display on mesh curvature='./exampleData/BV_GIFTI/GzipBase64/sujet01_Lwhite.shape.gii'; gcurv = gifti(curvature); % plot mesh with curvature figure; plot(gmesh,gcurv)","title":"Open Gifti File example"},{"location":"surfdata/formatref/#create-gifti-file-example","text":"% add release library to path addpath /home/chidi/repos/CFN/SurfaceFormat/library/gifti-release load mri D = squeeze(D); Ds = smooth3(D); g = gifti(isosurface(Ds,5)) h = plot(g); daspect([1,1,.4]); view(45,30); axis tight lightangle(45,30); set(h,'SpecularColorReflectance',0,'SpecularExponent',50) save(g,'mri.surf.gii','Base64Binary'); gmri = gifti('mri.surf.gii'); figure; plot(gmri)","title":"Create Gifti File example"},{"location":"surfdata/formatref/#viewing-the-xml-header","text":"The gifti file itself is basically a large xml file that describes th metadata and data contained for the surface. In ASCII format then it is simple enough to just open it in a text editor and peruse the contents. In some cases the data is encoded as Base64 binary and compressed which the gifti library can manage. In order to see the xml header for files that are not ascii then you can simple open the gifti and then resave it as an ascii gifti as follows surfacemesh='./exampleData/BV_GIFTI/GzipBase64/sujet01_Lwhite.surf.gii' gmesh = gifti(surfacemesh); save(gmesh,'asAscii.gii','ASCII');","title":"Viewing the XML header"},{"location":"surfdata/formatref/#nifti-2","text":"","title":"Nifti-2"},{"location":"surfdata/formatref/#references_1","text":"Brainder blog has a nice detailed descrption of Nifti-2 and Nifti-1","title":"References"},{"location":"surfdata/formatref/#cifti","text":"","title":"Cifti"},{"location":"surfdata/formatref/#data","text":"An assortment of CIFTI and GIFTI files are available from OSF: If you have the python client osfclient installed then use: osf -p hetgq fetch /Data/HCP .","title":"Data"},{"location":"surfdata/formatref/#python","text":"Possible free jupyter option is https://colab.research.google.com","title":"Python"},{"location":"surfdata/formatref/#matlab","text":"There are libraries available in matlab for working with CIFTI. This FAQ from the Human Connectome Project provides 3 approaches for loading CIFTI files. For HCP","title":"Matlab"},{"location":"surfdata/formatref/#approach-1-for-hcp-meg-data","text":"The FieldTrip toolbox provides a matlab library for working with CIFTI files. It is for use with HCP MEG Data. It is not advised for use with MRI data because it pads the CIFTI matrix with NaN values however it is a completely independent library. The field trip toolbox functions are prefixed by ft_ as in ft_read_cifti","title":"Approach 1 : For HCP MEG data"},{"location":"surfdata/formatref/#approach-2-for-hcp-mri-data","text":"From the HCP FAQ download just the following 3 matlab functions ciftiopen.m , ciftisave.m and ciftisavereset.m and place in your path. You will need need the gifti matlab library as well. You can download a release version gifti-zip from here . Unzip to a folder and in matlab add to your path as addpath /path/to/gifti We will also need access to wb_command","title":"Approach 2 : For HCP MRI data"},{"location":"surfdata/formatref/#approach-3-alpha-testing-for-hcp-mri-data","text":"This library is in alpha testing phase and is based on the Field Trip fieldbox. It should resolve the issues with Approach 1 but it is still strictly in development and so changes are possible. The field trip library that it is based on is also included in subfolder ft_cifti For this exploration we will proceed with Approach 3 git clone https://github.com/Washington-University/cifti-matlab.git","title":"Approach 3: Alpha Testing for HCP MRI Data"},{"location":"surfdata/formatref/#container","text":"Singularity and Docker containers with the HCP Workbench are available for exploring CIFTI data. The singularity image can be run as follows: singularity run --nv -B $PWD:/mnt workbench.sif --homedir /mnt wb_view The docker image can be run as docker run --rm --user=developer -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v $HOME/.Xauthority:/home/developer/.Xauthority -it --net=host --pid=host --ipc=host aacazxnat/workbench:0.1 wb_view","title":"Container"},{"location":"surfdata/formatref/#problems-with-opening-wb_view-using-docker-on-gpu-enabled-computer","text":"You will need to add Nvidia-Docker to your system. See Github for instructions for installation which are replicated below: distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit sudo systemctl restart docker Test nvidia-smi with the latest official CUDA image docker run --gpus all nvidia/cuda:10.2-base nvidia-smi Now try the gpu image: docker run --gpus all --rm --user=developer -e DISPLAY=$DISPLAY -v /usr/lib/nvidia:/usr/lib/nvidia -v /tmp/.X11-unix:/tmp/.X11-unix -v $HOME:/mnt -v $HOME/.Xauthority:/home/developer/.Xauthority -it --net=host --pid=host --device /dev/dri --ipc=host --privileged aacazxnat/workbenchgpu_runtime:0.1 --homedir /mnt wb_view","title":"Problems with opening wb_view using Docker on gpu enabled computer"},{"location":"surfdata/introformats/","text":"Lesson 8: Introduction to Surface-Based Formats Volume-based formats Magnetic Resonance (MR) images are typically acquired as a 3D volume matrix of intensity values which represent the strength of tisue signal at contiguous volume pixels otherwise known as voxels . MRI voxel data is usually acquired in Dicom format but converted to Nifti format for most day to day processing tasks as the later is a much more tractable format to use. A voxel location is specified by three indexes i , j and k which represent 3D cartesian space. A coordinate transform stored in the header of the Nifti allows one to map between the voxel space i,j,k and world coordinate space x, y, z in metric units (mm) What is a surface-based format Another approach to storing MRI data is to use a surface-based format . In this case tissue signal is recorded at distinct locations on the surface of an anatomical structure. These locations or vertices are specified by a single index which is a unique identifier for the vertex. Vertices are connected to 2 other vertices by an edge to create a triangular face . Many triangles are tesselated in a non-overlapping manner to create a mesh that covers the entire surface. Each vertex is associated with a real world 3D coordinate space location ( x,y,x ) just like a voxel which is usually in millimeter (mm) length units.See Appendix 1 for a comparison between Surface based models and traditional volume-based models that has been reproduced from a Freesurfer course online. Cortical surface reconstruction (image: Pirondini et al (2017) Computationally Efficient Algorithms for Sparse, Dynamic Solutions to the EEG Source Localization Problem Info To fully represent a cortical surface (e.g. the boundary between white matter and gray matter) as a 3D model, then the following information is usually stored in a Surface-Based Format by Neuroimaging software: A list of all the vertices in that surface with their x, y and z coordinates in mm units. For a 1mm T1w image, Freesurfer uses about 120,000 vertices to represent one of the hemispheres of the brain. A list of all faces in that surface where a face is a set of three interconnected vertices. For a 1mm T1w image, Freesurfer uses about 300,000 faces to represent one of the surfaces. There isn't necessarilly a predictable way to determine the number of faces from the number of vertices. For example see 2 different configurations below with the same number of vertices. There are two main surface models represented by surface-based formats: Meshes - these define the actual surface topology Overlays - these assign functional values to each vertex (e.g. thickness, curvature, myelin content, functional activation, BOLD signal, correlation etc..) When are Surface-Based formats appropriate? Surface-based formats are only really useful if you have high resolution anatomical images that allow you to segment the brain into distinct tissue types and/or anatomical structures. They have found most utility for the analysis of the cerebral cortex due to its highly folded geometry which can lead to partial volume errors under regular volumetric analysis pipelines. The cerebellum is also a potential candidate for surface based analysis but requires extremely high resolution anatomical scans to resolve. See this recent paper by Sereno et al 2020 . Why use Surface-Based Formats? Assuming you do have a good quality, high resolution (at least 1mm) T1w image (this is now accessible to most research groups), why would you want to bother working in surface space for the analysis of the cerebral cortex? Functional and anatomical validity The main reason for this is that the cortical sheet in the cerebrum and cerebellum appears to display functional differentiation and organization in distinct cortical areas which are best represented as a 2-dimensional surface Glasser et al, 2013 . Functional experiments with visual stimuli have allowed the organization of primary visual areas to be distinctly identified using surface-based methods [Sereno et al 1995] as patches on a 2D flat map. Volumetric approaches have struggled to reveal this functional organization in primary visual cortex. Processing accuracy and reliability The neurobiological validity Glasser et al, 2013 of cortical sheet representation as a 2-D surface ensures that processing steps take accurate account of spatial locations and are thus accurate and reliable. In volumetric space, due to the highly convoluted nature of the cortex, adjacent voxels may actually be very distant from each other in reality. Sulcal and gyral gaps are accurately captured by vertex notation and allow preprocessing steps like smoothing, filtering, correlations and any actions that require an accurate inventory of spatial locations being used to operate with less error due to partial volume confounds. Group analysis There seems to be a correlation between cortical folds and function. Surface models use these surface features (gyri and sulci) which show correspondence across subjects to improve inter-subject registration which increases the accuracy of group analysis. Economy/Efficiency of representation Large areas of brain tissue that are redundant to the analysis can be ignored. Thus the cortex can be represented using smaller file sizes in surface space. This makes the creation of dense connectomes feasible (albeit still computationally intensive) where they were barely manageable in voxel space. This also has an impact with respect to multiple comparisons in univariate whole-brain statistical anaysis. Note Another way that economy could be achieved in general is by using fewer vertices in regions where curvature is reduced so that slowly changing geometry is represented by larger triangles. In practice this is not done in neuroimaging as the uniform sampling of vertices is also important for modalities such as functional mri and so vertices are usually created at a fairly regular inteval of ~1mm in Freesurfer. The GIFTI Format The Gifti Data Format is used to represent surface-based data. A nice overview of GIFTI and CIFTI files is provided on the balsa website. The GIFTI specifications provide more details on the format itself. The GIFTI header describes the metadata of the surface information in xml format. The actual data is stored after the header. The GIFTI file can be in ascii or compressed base 64 format. The GIFTI format has the extension .gii . Mesh model captures the vertex locations and vertex neighbourhood in a *.surf.gii file Other derived files like *.func.gii and *.shape.gii and *.label.gii in combination with the surf.gii file, support derived measures like fMRI time series, curvature, and parcel labels respectively. Historical milestones Sereno Van Essen - Caret Dale and Fischl etc.. Obtaining Surface-based models today There are several approaches to creating an accurate surface model from a T1w image. The open source software FreeSurfer {Fischl 2012] is the most established and most widely used package and uses a number of sophisticated algorithms and procedures to take a T1w image, segment it into tissue classes and then identify the boundary points between these tissue clases which then form the surfaces that are the foundation of the surface model. Some caveats While the Freesurfer pipeline is usually adequate for most standard pipelines it is possible that if you are studying a less representative population like neonates, children or lesioned/sectioned brains [Makrapoulos 2018] for which Freesurfer is not optimized (for children under 5, white matter is unmyelinated so contrast between grey and white is ambiguous) then you may need to tweak the pipeline or find/develop alternative approaches. Indeed the HCP project worked with the Freesurfer team to develop a version of Freesurfer that could work with higher resolution T1w and T2w images using the --hires flag. The high-resolution layer-fMRI community is also split on their advocacy for freesurfer with some researchers using it and others deciding to create their own pipelines. See this dicussion by Renzo Huber Freesurfer Pipeline The freesurfer pipeline is the standard approach to creating surface models. We provide an overview of the pipeline here which are covered in detail by [Dale et al 1998] and [Fischl et al 1998]. Segmentation Bias-field correction etc. Freesurfer Outputs Freesurfer outputs surface and volume files. Some of these files .pial . .white , etc are in Freesurfers native surface format. Others are provided in the GIFTI format. The GIFTI matlab library is a versatile package that supports several surface models (like freesurfer, VTK and others) as well as standard GIFTI formats. The Ballad of Surface and Volume Oh Surface is Surface, and Volume is Volume, and never the twain shall meet, But there is neither Surface nor Volume, Border, nor Area, nor Origin - CIFTI The GIFTI format is able to manage most requirements for representing surface data however in some cases it is useful to be able to manage both surface and volume models together. For example if one wanted to calculate correlations between vertices on the surface and voxels in a subcortical structure like the hippocampus. The CIFTI format provides support for this scenario. The CIFTI format relies on the NIFTI-2 format as the original NIFTI is restricted to 32767 values in each dimension which is not sufficient to support high resolution surface models which can have as many as 164,000 vertices. The CIFTI format CIFTI format used to support both volume and surface data in the same file. Each CIFTI file with surface information is linked to an external gifti surf file which describes the vertex locations and topology. The CIFTI file header is stored in the NIFTI-2 extensions section in xml format and describes all the metadata for the data. The actual vertex and voxel data is stored in row-major format in the file. HCP Workbench Like freeview will allow us to view GIFTIs and CIFTIs. References Coalson, T. S., Van Essen, D. C., & Glasser, M. F. (2018). The impact of traditional neuroimaging methods on the spatial localization of cortical areas. Proceedings of the National Academy of Sciences, 115(27), E6356-E6365. Dickie, E. W., Anticevic, A., Smith, D. E., Coalson, T. S., Manogaran, M., Calarco, N., ... & Voineskos, A. N. (2019). ciftify: A framework for surface-based analysis of legacy MR acquisitions. Neuroimage, 197, 818-826. Glasser, M. F., Sotiropoulos, S. N., Wilson, J. A., Coalson, T. S., Fischl, B., Andersson, J. L., ... & Van Essen, D. C. (2013). The minimal preprocessing pipelines for the Human Connectome Project. Neuroimage, 80, 105-124. Appendix 1: Volume vs Surface Model This table was culled from this presentation . Volume Surface Uniform grid Non-uniform grid Voxel intersection of grid lines Vertex is intersection of triangles Columns, rows, slices Vertex index Voxel size/distance is fixed Distance between vertices ~ 1mm Voxel assigned a value Vertex assigned a value Voxel located at x,y,z (mm) Vertex located at x,y,x (mm)","title":"Lesson 8: Introduction to Surface-Based Formats"},{"location":"surfdata/introformats/#lesson-8-introduction-to-surface-based-formats","text":"","title":"Lesson 8: Introduction to Surface-Based Formats"},{"location":"surfdata/introformats/#volume-based-formats","text":"Magnetic Resonance (MR) images are typically acquired as a 3D volume matrix of intensity values which represent the strength of tisue signal at contiguous volume pixels otherwise known as voxels . MRI voxel data is usually acquired in Dicom format but converted to Nifti format for most day to day processing tasks as the later is a much more tractable format to use. A voxel location is specified by three indexes i , j and k which represent 3D cartesian space. A coordinate transform stored in the header of the Nifti allows one to map between the voxel space i,j,k and world coordinate space x, y, z in metric units (mm)","title":"Volume-based formats"},{"location":"surfdata/introformats/#what-is-a-surface-based-format","text":"Another approach to storing MRI data is to use a surface-based format . In this case tissue signal is recorded at distinct locations on the surface of an anatomical structure. These locations or vertices are specified by a single index which is a unique identifier for the vertex. Vertices are connected to 2 other vertices by an edge to create a triangular face . Many triangles are tesselated in a non-overlapping manner to create a mesh that covers the entire surface. Each vertex is associated with a real world 3D coordinate space location ( x,y,x ) just like a voxel which is usually in millimeter (mm) length units.See Appendix 1 for a comparison between Surface based models and traditional volume-based models that has been reproduced from a Freesurfer course online. Cortical surface reconstruction (image: Pirondini et al (2017) Computationally Efficient Algorithms for Sparse, Dynamic Solutions to the EEG Source Localization Problem Info To fully represent a cortical surface (e.g. the boundary between white matter and gray matter) as a 3D model, then the following information is usually stored in a Surface-Based Format by Neuroimaging software: A list of all the vertices in that surface with their x, y and z coordinates in mm units. For a 1mm T1w image, Freesurfer uses about 120,000 vertices to represent one of the hemispheres of the brain. A list of all faces in that surface where a face is a set of three interconnected vertices. For a 1mm T1w image, Freesurfer uses about 300,000 faces to represent one of the surfaces. There isn't necessarilly a predictable way to determine the number of faces from the number of vertices. For example see 2 different configurations below with the same number of vertices. There are two main surface models represented by surface-based formats: Meshes - these define the actual surface topology Overlays - these assign functional values to each vertex (e.g. thickness, curvature, myelin content, functional activation, BOLD signal, correlation etc..)","title":"What is a surface-based format"},{"location":"surfdata/introformats/#when-are-surface-based-formats-appropriate","text":"Surface-based formats are only really useful if you have high resolution anatomical images that allow you to segment the brain into distinct tissue types and/or anatomical structures. They have found most utility for the analysis of the cerebral cortex due to its highly folded geometry which can lead to partial volume errors under regular volumetric analysis pipelines. The cerebellum is also a potential candidate for surface based analysis but requires extremely high resolution anatomical scans to resolve. See this recent paper by Sereno et al 2020 .","title":"When are Surface-Based formats appropriate?"},{"location":"surfdata/introformats/#why-use-surface-based-formats","text":"Assuming you do have a good quality, high resolution (at least 1mm) T1w image (this is now accessible to most research groups), why would you want to bother working in surface space for the analysis of the cerebral cortex?","title":"Why use Surface-Based Formats?"},{"location":"surfdata/introformats/#functional-and-anatomical-validity","text":"The main reason for this is that the cortical sheet in the cerebrum and cerebellum appears to display functional differentiation and organization in distinct cortical areas which are best represented as a 2-dimensional surface Glasser et al, 2013 . Functional experiments with visual stimuli have allowed the organization of primary visual areas to be distinctly identified using surface-based methods [Sereno et al 1995] as patches on a 2D flat map. Volumetric approaches have struggled to reveal this functional organization in primary visual cortex.","title":"Functional and anatomical validity"},{"location":"surfdata/introformats/#processing-accuracy-and-reliability","text":"The neurobiological validity Glasser et al, 2013 of cortical sheet representation as a 2-D surface ensures that processing steps take accurate account of spatial locations and are thus accurate and reliable. In volumetric space, due to the highly convoluted nature of the cortex, adjacent voxels may actually be very distant from each other in reality. Sulcal and gyral gaps are accurately captured by vertex notation and allow preprocessing steps like smoothing, filtering, correlations and any actions that require an accurate inventory of spatial locations being used to operate with less error due to partial volume confounds.","title":"Processing accuracy and reliability"},{"location":"surfdata/introformats/#group-analysis","text":"There seems to be a correlation between cortical folds and function. Surface models use these surface features (gyri and sulci) which show correspondence across subjects to improve inter-subject registration which increases the accuracy of group analysis.","title":"Group analysis"},{"location":"surfdata/introformats/#economyefficiency-of-representation","text":"Large areas of brain tissue that are redundant to the analysis can be ignored. Thus the cortex can be represented using smaller file sizes in surface space. This makes the creation of dense connectomes feasible (albeit still computationally intensive) where they were barely manageable in voxel space. This also has an impact with respect to multiple comparisons in univariate whole-brain statistical anaysis. Note Another way that economy could be achieved in general is by using fewer vertices in regions where curvature is reduced so that slowly changing geometry is represented by larger triangles. In practice this is not done in neuroimaging as the uniform sampling of vertices is also important for modalities such as functional mri and so vertices are usually created at a fairly regular inteval of ~1mm in Freesurfer.","title":"Economy/Efficiency of representation"},{"location":"surfdata/introformats/#the-gifti-format","text":"The Gifti Data Format is used to represent surface-based data. A nice overview of GIFTI and CIFTI files is provided on the balsa website. The GIFTI specifications provide more details on the format itself. The GIFTI header describes the metadata of the surface information in xml format. The actual data is stored after the header. The GIFTI file can be in ascii or compressed base 64 format. The GIFTI format has the extension .gii . Mesh model captures the vertex locations and vertex neighbourhood in a *.surf.gii file Other derived files like *.func.gii and *.shape.gii and *.label.gii in combination with the surf.gii file, support derived measures like fMRI time series, curvature, and parcel labels respectively.","title":"The GIFTI Format"},{"location":"surfdata/introformats/#historical-milestones","text":"Sereno Van Essen - Caret Dale and Fischl etc..","title":"Historical milestones"},{"location":"surfdata/introformats/#obtaining-surface-based-models-today","text":"There are several approaches to creating an accurate surface model from a T1w image. The open source software FreeSurfer {Fischl 2012] is the most established and most widely used package and uses a number of sophisticated algorithms and procedures to take a T1w image, segment it into tissue classes and then identify the boundary points between these tissue clases which then form the surfaces that are the foundation of the surface model.","title":"Obtaining Surface-based models today"},{"location":"surfdata/introformats/#some-caveats","text":"While the Freesurfer pipeline is usually adequate for most standard pipelines it is possible that if you are studying a less representative population like neonates, children or lesioned/sectioned brains [Makrapoulos 2018] for which Freesurfer is not optimized (for children under 5, white matter is unmyelinated so contrast between grey and white is ambiguous) then you may need to tweak the pipeline or find/develop alternative approaches. Indeed the HCP project worked with the Freesurfer team to develop a version of Freesurfer that could work with higher resolution T1w and T2w images using the --hires flag. The high-resolution layer-fMRI community is also split on their advocacy for freesurfer with some researchers using it and others deciding to create their own pipelines. See this dicussion by Renzo Huber","title":"Some caveats"},{"location":"surfdata/introformats/#freesurfer-pipeline","text":"The freesurfer pipeline is the standard approach to creating surface models. We provide an overview of the pipeline here which are covered in detail by [Dale et al 1998] and [Fischl et al 1998]. Segmentation Bias-field correction etc.","title":"Freesurfer Pipeline"},{"location":"surfdata/introformats/#freesurfer-outputs","text":"Freesurfer outputs surface and volume files. Some of these files .pial . .white , etc are in Freesurfers native surface format. Others are provided in the GIFTI format. The GIFTI matlab library is a versatile package that supports several surface models (like freesurfer, VTK and others) as well as standard GIFTI formats.","title":"Freesurfer Outputs"},{"location":"surfdata/introformats/#the-ballad-of-surface-and-volume","text":"Oh Surface is Surface, and Volume is Volume, and never the twain shall meet, But there is neither Surface nor Volume, Border, nor Area, nor Origin - CIFTI The GIFTI format is able to manage most requirements for representing surface data however in some cases it is useful to be able to manage both surface and volume models together. For example if one wanted to calculate correlations between vertices on the surface and voxels in a subcortical structure like the hippocampus. The CIFTI format provides support for this scenario. The CIFTI format relies on the NIFTI-2 format as the original NIFTI is restricted to 32767 values in each dimension which is not sufficient to support high resolution surface models which can have as many as 164,000 vertices.","title":"The Ballad of Surface and Volume"},{"location":"surfdata/introformats/#the-cifti-format","text":"CIFTI format used to support both volume and surface data in the same file. Each CIFTI file with surface information is linked to an external gifti surf file which describes the vertex locations and topology. The CIFTI file header is stored in the NIFTI-2 extensions section in xml format and describes all the metadata for the data. The actual vertex and voxel data is stored in row-major format in the file.","title":"The CIFTI format"},{"location":"surfdata/introformats/#hcp-workbench","text":"Like freeview will allow us to view GIFTIs and CIFTIs.","title":"HCP Workbench"},{"location":"surfdata/introformats/#references","text":"Coalson, T. S., Van Essen, D. C., & Glasser, M. F. (2018). The impact of traditional neuroimaging methods on the spatial localization of cortical areas. Proceedings of the National Academy of Sciences, 115(27), E6356-E6365. Dickie, E. W., Anticevic, A., Smith, D. E., Coalson, T. S., Manogaran, M., Calarco, N., ... & Voineskos, A. N. (2019). ciftify: A framework for surface-based analysis of legacy MR acquisitions. Neuroimage, 197, 818-826. Glasser, M. F., Sotiropoulos, S. N., Wilson, J. A., Coalson, T. S., Fischl, B., Andersson, J. L., ... & Van Essen, D. C. (2013). The minimal preprocessing pipelines for the Human Connectome Project. Neuroimage, 80, 105-124.","title":"References"},{"location":"surfdata/introformats/#appendix-1-volume-vs-surface-model","text":"This table was culled from this presentation . Volume Surface Uniform grid Non-uniform grid Voxel intersection of grid lines Vertex is intersection of triangles Columns, rows, slices Vertex index Voxel size/distance is fixed Distance between vertices ~ 1mm Voxel assigned a value Vertex assigned a value Voxel located at x,y,z (mm) Vertex located at x,y,x (mm)","title":"Appendix 1: Volume vs Surface Model"},{"location":"surfdata/prac81/","text":"Practicum 8.1 - Surface-Based Data Formats Synopsis Surface-based data formats represent neuroimaging data in two main forms namely meshes and overlays . In both cases anatomical locations are represented by vertices which are arbitrary points in 3D space. Surface meshes store 2 main types of information - the Cartesian location (x, y, z) of each vertex in 3D space and the triple of vertex indices that make up a triangular face on the mesh. Surface overlays store attributes for each vertex. These attributes could be single value scalars like the cortical thickness at a vertex or a series of values like a time series from an fMRI or EEG experiment. Objectives This practicum will introduce you to working with surface-based data (meshes and overlays) using the GIFTI and CIFTI file formats. By the end of this practicum you should Be able to access the metadata of GIFTI and CIFTI files to determine the properties of the surface under study Be able to directly access the image data in a GIFTI and CIFTI file and work with it Be able to change the data in a GIFTI and CIFTI file and create a new file with your changes Visualize your GIFTI and CIFTI files using the workbench tool Note Some of the tasks you will accomplish with this practicum will not ordinarily be part of your typical neuroimaging analysis workflow. For example you will rarely if ever need to change the shape of a surface mesh. However the intention of these exercises is to cement your understanding of these formats by giving you hands-on experience with manipulating them albeit in somewhat artificial scenarios. Downloads/Installation This practicum requires the following materials/folder setup for successful complete: A reasonably recent version of matlab . The practicum was tested on version R2020a (linux), R2019a (windows) and R2018b (Mac) but should hopefully work on older versions as well. A copy of the HCP's Connectome workbench . We will be using wb_view for viewing CIFTI and GIFTI files. The install is a zip file that you extract and the executable should be available after extraction. Depending on your platform you may need to double-click the wb_view icon or add it to your path to access it via the command line. Copies of workbench have also been packaged up at the OSF site for this lesson under the 08-Surface-Based-Formats/Software folder which you can access if you have trouble getting to the connectome site. There are 4 zip files with names that start with workbench-.. for different OS platforms. The Matlab practicum files, Matlab libraries and data for this practicum are also all available at the OSF site folder under the 08-Surface-Based-Formats/practicum81 folder in a zip file practicum81.zip . Download this zip file to your local computer and extract. Your folder structure should look like this after extraction. \u2514\u2500\u2500 practicum81 \u251c\u2500\u2500 DATA \u2502 \u2514\u2500\u2500 HCP \u251c\u2500\u2500 matlab-library \u2502 \u251c\u2500\u2500 cifti-matlab \u2502 \u251c\u2500\u2500 gifti-release \u2502 \u251c\u2500\u2500 helper-functions \u2502 \u2514\u2500\u2500 xml2struct \u251c\u2500\u2500 task_002_giftisurf.m \u251c\u2500\u2500 task_003_giftioverlay.m \u251c\u2500\u2500 task_004_cifti.m \u2514\u2500\u2500 task_005_cifti.m Provenance Matlab libraries The Matlab libraries used in this practicum are described below: The HCP cifti-matlab library was in alpha-testing phase at the time of creating this practicum and so is liable to change in the future though hopefully not significantly. The version provided had its last commit 1e0ca9fbb0f9a8105d144ec6bc0de55b84d1b528 on Wed Sep 2 15:42:55 2020 -0500 . Please refer to the cifti-matlab repository for announcements on when the alpha-testing is complete. In the meantime if you use this code for your own research then please appreciate that you do this at your own risk and there is a chance of errors. [ Update on 11/2/2020, the cifti-matlab library was released as a beta version 2.0.0 for public testing on 9/30/2020. Comments from Tim Coalson summarizing the benefits of this version are available here ] The gifti library was downloaded as a zip file gifti-master.zip , extracted and renamed to gifti-release xml2struct was downloaded directly from the mathworks file-exchange. helper-functions mostly contains custom functions written in-house to support tasks 4 and 5. The function read_nifti2_hdr.m is a private function from the FieldTrip toolbox to help with reading NIFTI-2 files. Data We will be using a subset of data from the Human Connectome Project's Young Adult (YA) 1200 Subject Release . Specifically data from subject 100307. The HCP has terms of reference for using their data. To use HCP data please sign on to the HCP website and register for free. Miscellaneous files The documentation for this practicum has also been bundled with the practicum materials as prac81.pdf to mitigate against internet connectivity problems. The other four matlab files task_002_giftisurf.m , task_003_giftioverlay.m , task_004_cifti.m and task_005_cifti.m are needed to undertake the respective tasks below. Task 1: Use wb_view to visualize GIFTI surfaces Open wb_view either by double-clicking the wb_view or wb_view.exe icon in your windows manager or by entering this in the command line if you have set up your paths in the command line interface correctly. When wb_view opens click the skip button in the dialog to get to the main screen From the top menu choose File > Open File and change the dropdown on Files of type: to Surface Files (*.surf.gii) Now navigate to practicum81/DATA/HCP/100307/MNINonLinear/Native/ You will be presented with a host of files with the left mouse button click on 100307.L.pial.native.surf.gii Now pressing the CTRL key (Command key on Mac) again click with the left mouse button on 100307.R.pial.native.surf.gii to select both surfaces. You should see that both files are reflected in the File name textbox. The pial that these surfaces refer to is the surface boundary between grey matter and CSF in the outer cortex. Now click Open , you should see the left and right pial surfaces displayed with lateral and medial views Information In several exercises we make use of data on the subject's native mesh as in the example above with the 100307.L.pial.native.surf.gii and 100307.R.pial.native.surf.gii surfaces. It's important to mention that most of the HCP data is however not on the native mesh but on a standard fs_LR mesh on which the left and right hemispheres of all subjects have been aligned using the MSM-All surface-based registration approach Glasser et al, 2016 . There are two important standard spaces used in the HCP, namely, 164k_fs_LR which is a high resolution space of 163,842 vertices per hemisphere (at approx 1mm resolution) and the 32k_fs_LR space which is a lower resolution space of 32492 vertices per hemishere (at approx 2mm resolution). We will use an example of the latter in task 5. For more details about these standard meshes consult Glasser et al, 2013 and the HCP Wiki Page At the top of the application change from (1) Montage to (2) All by clicking on the tab. You should see the left and right pial surfaces aligned on the screen. Use your mouse left button to rotate the brain around and your scroll wheel to zoom in and out We can get a quick overview of these surfaces by choosing Surface > Information . We can see that there are 130879 vertices and 261754 faces in the left hemisphere pial surface. For the rest of this exercise we just want to work with left hemisphere so we will uncheck the display for the right cortex in the (2) All tab. Switch on edge view by clicking the Surface menu option. Choose Properties and change the Drawing Type to Links (Edges) . Zoom in a little closer to see the triangular tesselation. identify any vertex you like and click on it. For this example I have stumbled upon vertex 119115 which I have decided to probe. A dialog with title Information should pop up with vertex information. This dialog provides information about the vertex's hemisphere, label and its anatomical location in 3D Cartesian (x,y,z) coordinates. Notice also that the white vertex symbol icon is much larger than the width of the edges. Choose Properties in the information dialog ( Properties is under Contra ID in the dialog) and change the Symbol Diameter and Most Recent ID Symbol Diameter to a suitable size. We chose 0.5mm for our example. You can also change the ID Symbol Color to another value. Once you have selected a vertex we can now try and identify its neighbors. For the first pass through this practicum you should avoid choosing a vertex on the medial wall as this will cause difficulties in task 4. To be super safe then just replicate the vertex we have chosen or choose a vertex that is easilly accessible on the lateral surface. The medial wall contains vertices that are not actually on the cortical surface (actually these medial-wall vertices cut through subcortical grey matter structures and the Corpus callosum) but are required to maintain the spherical topology of the surface. Select all the neighbouring vertices (all vertices connected by an edge to your chosen vertex). These should show up in the information screen. You can select and copy the text from this screen and place it in a text file. We will need this information for the next Task. You can leave wb_view open but minimized and proceed to the next task. Task 2: Use Matlab to manipulate GIFTI Surfaces In this task we will be loading in the GIFTI surface 100307.L.pial.native.surf.gii which we visualized above and identifying the vertex and vertex neighbours we chose. We will then change the local shape of the surface by nudging these vertices by a random 3D translation and then save a new copy of the surface which we will visualize again using wb_view . Open Matlab and navigate to your the practicum81 folder containing task_002_giftisurf.m . Double click this file to open it in Matlab. We will run individual sections of the Matlab code by right-clicking on each section (highlighted yellow when selected) and selecting Evaluate Current Section . Select the 1st section and choose Evaluate Current Selection to run. This will ensure that Matlab can find the gifti-release library which we need to open and save GIFTI files. If you haven't set the right path to the gifti-release library then you will see an error like this: Warning: Name is nonexistent or not a directory /home/chidi/Downloads/ Practicum81/../matlab-library/gifti-release Now right-click and run Section A . If the variable mysurf is not pointing to the correct location for the data then you will get an error similar to this: Reading Surface ../DATA/HCP/100307/MNINonLinear/Native/100307.L.pial.native.surf.gii Error using xml_parser Cannot read XML document This section will plot a 3D model of the surface which can be rotated with the mouse. This is just to demonstrate that you can visualize your surface mesh within Matlab as well. You can close these windows whenever you want. you should also see that the expected number of faces and vertices are also printed to the screen. This information has been obtained using the gifti-release libraries gifti() function. The information is stored in the structure mysurfleft_mesh mysurfleft_mesh = struct with fields: faces: [261754\u00d73 int32] mat: [4\u00d74 double] vertices: [130879\u00d73 single] Note One source of potential confusion is that the gifti Matlab library indexes vertices starting from 1 but wb_view and the GIFTI specification start indexing from 0. So the 1st vertex has an index of 0 in wb_view but has an index value of 1 in Matlab . The faces matrix contains the 3 vertex indices that are joined together to form a face on each row. In Matlab we can view the first 4 faces as follows: mysurfleft_mesh.faces(1:4,:) 1 2 3 4 3 2 1 56 2 57 2 56 We see that the first face (row 1 above) comprises vertices 1,2 and 3 which are joined together by edges. Perhaps surprisingly we see that 56 and 57 are also joined to 2 to form the fourth face. This reinforces the fact that the vertex index is just a label and does not predict spatial proximity. Remember that this uses Matlab's indexing conventions. So wb_view would specify the first face as consisting of vertices 0,1 and 2. The vertices matrix provides the anatomical location of each vertex. Looking at the first 4 vertices, we see that vertex 3 (i.e. vertex 2 in wb_view convention) has a location of -13.3332 mm, -106.5394 mm, -5.5976 mm mysurfleft_mesh.vertices(1:4,:) -13.0844 -106.5673 -5.1778 -13.5373 -106.6587 -5.1817 -13.3332 -106.5394 -5.5976 -13.7358 -106.5008 -5.7497 Now run the next section B to find the neighbours of the vertex you located in task 1 above. Remember to change the vertex on line 25 ciftivind= to point to your vertex. If all goes well then you should see the same vertices you observed in wb_view . As already mentioned wb_view (and the GIFTI XML) start indexing vertices from 0 while the gifti matlab library starts indexing from 1. So we need to add 1 to the index we obtained from wb_view to create the correct index for use in Matlab. This is already accomplished for you in the code on line 29 by matlabvind=ciftivind + 1; The code looks at all the columns of the faces matrix and picks out the lines that include our vertex and then does a union to create a set of vertices without duplication. The faces matrix contains a row for every triangular face. So below for example, the vertices 119116, 119106 and 119115 are connected as a face. Find 119116 in column 1: 119116 119106 119115 119116 119128 119129 119116 119129 119117 Find 119116 in column 2: 118448 119116 119115 119106 119116 119107 Find 119116 in column 3: 118448 118449 119116 118449 119128 119116 119117 119107 119116 Amalgamate all the vertices and remove duplicates: 118448 118449 119106 119107 119115 119117 119128 119129 Now we will run section C to perturb each neighbouring vertex by a random number between 2 and -2 mm. Note: This scenario will rarely happen in real-world analysis but has been designed to give you insight into the internal workings of surface-based formats. Packages like FreeSurfer do allow you to manually manipulate surfaces to correct for errors in initial modelling and this exercise demonstrates how this would work in principle. This code goes through a loop and for each vertex obtains the x, y and z anatomical location information and perturbs it by a random number. We then create a new GIFTI file called amended.100307.L.pial.native.surf.gii in your current folder which contains the changed vertex locations in a new surface. Now we will return to wb_view to view your new GIFTI file. Open your amended GIFTI surface amended.100307.L.pial.native.surf.gii that was created in the Practicum81 folder in wb_view The original 100307.L.pial.native.surf.gii that was obtained from the folder Practicum81/DATA/HCP/100307/MNINonLinear/Native/ should already be loaded into wb_view if it was not closed previously. If not then open this GIFTI file as well. We will be comparing both of them in wb_view. By default there are two tabs provided when you first open wb_view these are (1) Montage and (2) All You can click on any of the tabs and change the View to Surface by clicking the corresponding radio button. It's also possible to create a new tab and change the view to Surface there. Ensure that under Brain Structure and Surface that the first drop down box is set as All or as CortexLeft and that the second drop down is set to our amended.100307.L.pial.native.surf.gii as the selected surface. let's locate our original vertex from task 1. The sequence of steps to take will depend on the state your GUI is in based on subsequent actions. The steps below should hopefully get you there regardless. Click on Window and then identify and click on the identify Surface Vertex and enter the vertex Index. For our example we used 119115 . If you have several vertices displayed which are confusing you can clear all vertices by clicking on RID button on the Information Properties dialog. After clicking apply it might also help to change the size and color of the vertex symbol to a larger value and/or more vivid color respectively if you haven't already done so previously. You may also have to zoom out and/or rotate the brain if you chose a vertex hidden from the lateral view. You should be able to spot your vertex. Now Change the Surface properties to Link(Edges) (if not already changed from previous steps) and zoom in closer to see how the pertubations you made to the neighboring vertices have changed the shape of the mesh. You will probably need to change the symbol diameter to something smaller. You should notice that the spatial location of the vertices has changed. You can contrast this with the original by clicking back and forth on the dropdown in the tab (under Brain Structure and Surface ) switching between the original surface and your amended surface. You can also click on a Montage view to see both surfaces side by side (You will need to deselect either the medial or lateral checkbox to see the vertex and neighbours properly side by side). You will also need to select both surfaces as different layers in the montage selection. You should confirm that you have successfully changed the shape of your surface. We won't dwell too long on the next two sections but these sections give you more insight into the XML structure of the GIFTI structure so that you can compare that with the data provided by the gifti matlab library. Essentially this library just reads this XML in the GIFTI to create the faces and vertices structures that you have just looked at. We will return to the Matlab file to run section D to view the GIFTI metadata in a browser to see exactly how the XML is used to represent the GIFTI format. All this section does is re-save the GIFTI in ASCII format as mygifti_allxml.gii so that most text editors can view it conveniently. After running section D, you can now simply open the created mygifti_allxml.gii file directly in your browser or by using VScode if it does not open automatically in firefox. (you may get an error on windows that states 'firefox is not recognized..' - this can be completely ignored and the command console that opens with this message can be closed) Some browsers like Chrome may require you to explicitly rename this as an xml file mygifti_allxml.xml in order to view the XML with helpful highlighting and the ability to collapse and expand sections. You can collapse and expand XML sections in the browser by clicking on the -/+ signs respectively. Notice that the vertex anatomical location information is represented as a DataArray with an Intent name of NIFTI_INTENT_POINTSET . The number of vertices is captured by Dim0=\"130879\" . The triangular faces are represented in the second data array as NIFTI_INTENT_TRIANGLE . Expand the Data tag in the NIFTI_INTENT_POINTSET dataarray and then look at the surface mesh vertex information that is loaded in by the gifti library. Notice how the columns are loaded in first before the rows. This is known as Column Major Order and is specified also in the XML as a DataArray attribute as <DataArray ArrayIndexingOrder=\"ColumnMajorOrder\".. Notice also that for the faces defined as NIFTI_INTENT_TRIANGLE in the second DataArray that the vertices are indexed starting from zero and are also stored using Column Major Order. Section E accomplishes the same as Section D except that now the xml data is loaded into a Matlab structure. You can skip this section as all it does is corroborate what you have already seen above. It is divided into 2 parts, E1 and E2 Run Section E1 to fix the xml in the GIFTI. We need to do this as unfortunately there is a slight problem reading the XML file into Matlab as the 2nd line which starts <!DOCTYPE causes problems with the parser. The line sed 2d... in the Matlab file is written to delete this line on unix and save a new text file called mygifti_xml.gii which is then parsed. In windows you will get an error that states 'sed is not recognized..' - this can be completely ignored and the command console that opens with this message can be closed). You will need to use powershell instead if its available or alternatively manually open the xml file in a text processor and delete the 2nd line. In Matlab you can invoke a powershell shell as follows: !powershell After powershell loads enter the command below to remove the 2nd line: Get-Content mygifti_allxml.gii | Where {$_ -notmatch 'DOCTYPE'} | Set-Content mygifti_xml.gii Then return to Matlab by typing exit Now run Section E2 to load the XML into a Matlab structure. You can now identify the same XML structures as we saw above in the browser. For example myXML.GIFTI.DataArray{2}.Data gives us the faces information for the surface: myXML.GIFTI.DataArray{2}.Data = Text: '0 3 0 56 0 0 1 1 2 2 2 72 3 3 4 4 6 14 6 6 6 11 Task 3: Use Matlab to manipulate and visualize GIFTI overlays We have been able to work with and manipulate GIFTI surface files . In this task we will now look at an example of a GIFTI overlay that maps cortical thickness values onto the pial vertices and we will attempt to manipulate and visualize this overlay. If wb_view is already open from previous tasks then you can use the File > Save/Manage Files functionality to selectively close files. Select to close the Right Pial and the amended Left pial. We will be using the original Left pial in this task. Alternatively if you want to start from scratch then just choose File > Close all files in wb_view load in the surface 100307.L.pial.native.surf.gii again from Practicum81/DATA/HCP/100307/MNINonLinear/Native/ if not already open. Click on Surface to view the Left hemisphere. Now load in the cortical thickness overlay 100307.L.thickness.native.shape.gii from the same location. You will need to change the Files of type field to Metric Files (*.func.gii *.shape.gii) or Any File (*) to be able to select the thickness overlay. Activate the layer by clicking on the checkbox alongside it. Also click on the color bar to see the range of cortical thicknesses across the brain and their associated colors. Now locate the vertices you looked at previously (using Window > identify ) and find their cortical thicknesses by looking at the information dialog. Notice that for our central vertex (119115 in this case) we have a cortical thickness of 3.54125 mm. We will now open this overlay in Matlab, change the cortical thickness of our target vertex and its neighbors and visualize the changed overlay in wb_view Open the file task_003_giftioverlay.m in matlab. As before run each section in Matlab separately to follow along with what is happening in the code. First run the addpath .. section to ensure that the gifti library is accessible and that all Matlab windows are closed and variables cleared. In section A we load in the surface as before and now also load in the cortical thickness overlay. Notice that the overlay and the surface match as the number of vertices is equal to the number of thickness values. Reading Surface ./DATA/HCP/100307/MNINonLinear/Native/100307.L.pial.native.surf.gii Surface has 261754 faces Surface has 130879 vertices Reading Overlay ./DATA/HCP/100307/MNINonLinear/Native/100307.L.thickness.native.shape.gii Overlay has 130879 values The overlay has all its thickness values for each vertex stored in a cdata matrix: myoverlayleft = struct with fields: cdata: [130879\u00d71 single] Using the gifti library's overloaded plot command we also visualize the surface in one window and also observe overlay plotted on the surface in another window with a color scale used to designate varying thickness. In Section B we see a print out of the cortical thickness values at our neighbouring vertices which should match the values we identifed in wb_view . In Section C we set the thicknesses of our chosen vertex and its neighbors to a uniform value of 2mm and then save a copy called amended.100307.L.thickness.native.shape.gii . Return to wb_view and open the amended.100307.L.thickness.native.shape.gii overlay. Compare this with the original 100307.L.thickness.native.shape.gii overlay and confirm that all the vertices now have a thickness value of 2mm. You can change what overlays are activated and the precedence in which they are displayed by selecting different files and/or toggling the visibility of the layer. You may also want to change the Drawing Type in Surface > Properties to Triangles to see a clear thickness change in the patch of cortex as you toggle between layers. In addition you may also want to view the amended cortical thickness overlay amended.100307.L.thickness.native.shape.gii on your amended pial surface amended.100307.L.pial.native.surf.gii or on other surfaces that are available in the same directory. These surfaces are all in vertex correspondence for the subject 100307. In section D and E you can again study the XML structure of the overlay as you did in task 2 above to see how it corresponds to the gifti structure loaded into Matlab. Task 4: Use Matlab to manipulate and visualize CIFTI files So far we have learned to work with GIFTI files. In this penultimate task we are going to work with CIFTI files which combine both volume data for subcortical structures and surface data for cortical structures into one file. Unfortunately the HCP's Matlab library was still in Alpha testing at the time of creating this practicum and so we need to be aware that there is a slight chance that the coding conventions used here may change however the overal conceptual approach should remain the same. We will work with HCP data again and this time with CIFTI cortical thickness data 100307.thickness.native.dscalar.nii from the left and right cortex. To visualize CIFTI data in wb_view we need a GIFTI surface file that is associated with the CIFTI data. Close all loaded surfaces and overlays if wb_view is already open. Open wb_view and load in GIFTI surface 100307.L.midthickness.native.surf.gii - this surface is midway between the pial surface we used previously and the white surface. Now load in the midthickness surface for the right hemisphere 100307.R.midthickness.native.surf.gii . Click on All view to see both hemispheres aligned in the view. You can rotate the whole brain to a perspective that works for you. Now open 100307.thickness.native.dscalar.nii - you will need to specify Files of type: (*.dscalar.nii) to be able to select this file. Activate the layer by clicking on the checkbox and notice that this CIFTI file has cortical thickness information for both hemispheres. lets return to Matlab and open the Matlab file task004_cifti.m and execute the prelim section to load in the Matlab libraries that we need. In Section A we will briefly explore the overlay 100307.thickness.native.dscalar.nii as a NIFTI-2 file. We will use a private function read_nifti2_hdr.m which is packaged as part of the FieldTrip Matlab library. The diagram below shows how a NIFTI-2 file is used to package up this CIFTI. The extra space used in the NIFTI-2 file to store the cifti-XML is called a cifti extension or extension . We can confirm that the size of the CIFTI file in bytes is equal to the size of the voxel offset field plus the data size in bytes: >> filesize = 2525056 >> nii2.vox_offset 1531104 >> nii2.dim 6 1 1 1 1 1 248488 1 where the value of nii2.dim[7] = 248488 is the data size in floats which is equal to 4*248488=993952 in bytes. The XML is thus embedded in the extent section between the header and the data. It is this section that the cifti-matlab library reads to determine exactly how the surface and volume models are set up. Run Section B to explore the CIFTI XML file in a Matlab structure and in a browser. Notice that like the gifti overlay this cifti overlay has data for both cortices in a cdata structure which can be accessed as ciiall.cdata >> ciiall ciiall = struct with fields: metadata: [1\u00d73 struct] diminfo: {[1\u00d71 struct] [1\u00d71 struct]} cdata: [248488\u00d71 single] The xml data has also been stored in a CIFTI structure >> ciixml ciixml = struct with fields: CIFTI: [1\u00d71 struct] The information stored in the above two structures matches the XML data that can be visualized in the browser. The code has been set up to open the CIFTI XML automatically in firefox if it is installed in your path but this may not work on your system without some tweaking. You can also simply view the XML directly in a browser or in VScode by opening the file debug.xml that is created in your current folder. Notice that the first dimension is CIFTI_INDEX_TYPE_SCALARS which means that each vertex/voxel has a single scalar value which in this file is the cortical thickness. Also notice that the second dimension is CIFTI_INDEX_TYPE_BRAIN_MODELS which is used to map every voxel/vertex in a cortical or subcortical structure. Just the left cortex CIFTI_STRUCTURE_CORTEX_LEFT and the right cortex CIFTI_STRUCTURE_CORTEX_RIGHT are represented in this file. Information for each cortex can be obtained by looking at the diminfo fields. Here we look at the model for the CORTEX_LEFT which is contained in diminfo{1}.models{1} >> ciiall.diminfo{1}.models{1} ans = struct with fields: start: 1 count: 125197 struct: 'CORTEX_LEFT' type: 'surf' numvert: 130879 vertlist: [1\u00d7125197 double] Unlike the GIFTI Matlab library that indexed vertices starting from 1, this CIFTI library uses an index to index the vertices! This is a little complicated but in essence if we look at the vertlist structure for the first three vertices in the left hemisphere we see that they are correctly identified as 0,1 and 2. ciiall.diminfo{1}.models{1}.vertlist(1:3) ans = 0 1 2 As already mentioned, the data structure ciiall contains all the cortical thickness data for both hemispheres in ciiall.cdata indexed starting from 1 to 125197 for the left cortex and from 125198 to 248488. These indexes are used as pointers into a vertex list vertlist which is stored for each surface. So the cortical thickness values for the first three vertices 0,1,2 in the Left hemisphere are obtained as follows: ciiall.cdata(1:3) ans = 3\u00d71 single column vector 2.201649 2.323979 2.41378 There however appears to be an inconsistency with the total number of vertices allocated to the overlay. In the last task we had 130879 vertices in the left hemisphere but here we have 125197 vertices assigned (roughly about 5600 vertices fewer). What's going on? Essentially the HCP CIFTI files do not store data for the medial wall of each cortex. We saw the medial wall in the first task. We will identify these medial vertices and visualize a few of them on the left cortex. Run section C to print out all the vertices that are missing from the left cortex. These are the medial vertices. There should be 5682 vertices in the medial wall displayed to screen. Select one or a few of them and using the identify tool in wb_view try to see where they are. You will need to use the surface view and visualize the left cortex only so you can see these vertices. Some of these are easier to spot than others. In section D and E we do what we did in task 3 and will now go ahead and change the value of a vertex and its neighbors to a constant value of 2mm cortical thickness. The code uses a vertex value of 119115 but of course you should use any vertex you like except for the medial vertices which the CIFTI file does not track. Open the amended.100307.thickness.native.dscalar.nii that is created in your current directory in wb_view and navigate to the vertex to confirm that changes have been made. An additional section at the end of the file provides code to replace sections E and F, which shows how the cifti-matlab helper function cifti_diminfo_dense_get_surface_info allows us to more efficiently identify the vertices on the LEFT_CORTEX and map them to their corresponding data values in the ciiall.cdata matrix. This is the preferred approach to accessing data associated with surface structures in the cifti file. The equivalent function for accessing sub-cortical structures is cifti_diminfo_dense_get_volume_structure_info . Please refer to the cifti-matlab library for additional convenience functions. Task 5: Use Matlab to manipulate and visualize CIFTI files with volumes and surfaces combined We now look at a slightly more complex CIFTI file which stores fMRI data for Surface and Volume structures simultaneously. We will be working in the HCP's standard space for this and so will close all the files in wb_view and then load in the surfaces which we will be using for this exploration. Click on File > Open File and navigate to /DATA/HCP/100307/MNINonLinear/fsaverage_LR32k/ and open the surfaces 100307.R.midthickness.32k_fs_LR.surf.gii and 100307.L.midthickness.32k_fs_LR.surf.gii select the All view and then load in the fMRI dtseries.nii CIFTI file rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii from the location DATA/HCP/100307/MNINonLinear/Results/rfMRI_REST1_LR/ . Change the file overlay from dynconn - rfMRI_REST1_LR_Atlas_hp2000_clean.dynconn to the dtseries rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii and switch on the overlay to see the first volume of the resting state fMRI series which has an index of 1 and a Map value of 0 seconds. Rotate the brain to get a better view of the fMRI data. Notice that there are values in both the cortex (surface-based data represented by vertices) and in the subcortex (volume-based data represented by voxels). To navigate the volume-based data more conveniently you can click on the Volume view to access the traditional volume viewing interface. In Slice Plane pane (not the View Pane!) you can select All to view Axial, Parasagittal and Coronal planes at the same time. Again you will need to switch on the overlay and choose rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii over the dynconn... overlay. You can move through volume slices by holding the left mouse button and dragging the mouse. You can zoom in by using the mouse scroll wheel. To help with spatial awareness you can load in a NIFTI-1 T1w volume. Open T1w_restore_brain.nii.gz which is available at /DATA/HCP/100307/MNINonLinear . Activate the overlay so that it is visible. In both the voxel and surface view, individual fmri instances at each TR can be stepped through by clicking on the index arrows. You can also navigate to a specific timepoint in the series by clicking on the map drop down and selecting the time instance of interest. One nice application within wb_view is the ability to interactively view functional connectivity. To see this in action Let us activate the All tab. We can then load in Freesurfer's Destrieux atlas which has been converted to gifti format. The left and right hemisphere atlases are called 100307.L.aparc.a2009s.32k_fs_LR.label.gii and 100307.R.aparc.a2009s.32k_fs_LR.label.gii and are available here /DATA/HCP/100307/MNINonLinear/fsaverage_LR32k/ . Activate all three overlays in the All view and from top to bottom select the Dynconn - rfMRI.. layer, then the Left and Right atlas label files. Now click on any brain area and you will see the functional connectivity of every other vertex to that vertex which has been calculated dynamically. If you spin the cortex around you will also see that the connectivity is also calculated for voxels as well. The areas which are functionally connected are displayed with colors that represent higher correlation. So in the example below all the yellow areas are functionally connected. For the above we used a different color map which is accessible from the little wrench symbol. You can also view the scale used for the color bar by clicking the multi-colored bar. Back to Matlab now so we can explore a little more how this CIFTI file stores both volumes and surfaces in the same NIFTI-2 file. Open task_005_cifti.m and run the first section as usual to clear the workspace and add the necessary libraries to our path. Run Section A to see how the CIFTI XML has different sections for surfaces and for voxels. Note that each volume structure has an individual label e.g CIFTI_STRUCTURE_ACCUMBENS_LEFT. As you did in Task 4 you can query the XML directly in the structure ciixml and also compare this with the data that has been stored in the data structure ciiall by the HCP Matlab library. The output that is displayed in the command window is obtained by going through the XML file. It shows that information about voxels and vertices are stored at the same time in the XML file and that the actual overlay data for each vertex and voxel is indexed sequentially in the NIFTI-2 file. So for example with our CIFTI file if we read off a value at row position 59695 in the data array then this will be a value in AMYGDALA_LEFT (between 59688 and 60002) ... Model 1 is a surface of CORTEX_LEFT with 29696 verts valued out of 32492 vertices; start from 1 and end at 29696 Model 2 is a surface of CORTEX_RIGHT with 29716 verts valued out of 32492 vertices; start from 29697 and end at 59412 Model 3 is a volume of ACCUMBENS_LEFT with 135 voxels; start from 59413 and end at 59547 Model 4 is a volume of ACCUMBENS_RIGHT with 140 voxels; start from 59548 and end at 59687 Model 5 is a volume of AMYGDALA_LEFT with 315 voxels; start from 59688 and end at 60002 Model 6 is a volume of AMYGDALA_RIGHT with 332 voxels; start from 60003 and end at 60334 ... The code has been set up to open the CIFTI XML automatically in firefox if it is installed in your path but this may not work on your system without some tweaking. You can also simply view the XML directly in a browser or in VScode by opening the file debug.xml that is created in your current folder. In section B , we will now take an arbitrary vertex 17617 in the Right hemisphere and plot the fmri data in that vertex along with its closest neighbors. Observe that we have to check for existence of the neighbors that we find in the available list of vertices because one of the neighbors might actually be in the medial wall. This is achieved for you using the code neighindex=ismember(visverts,allverts); which takes allverts which includes vertex 17617 as well as its close neighbours and compares it against all the visible vertices that are not on the medial wall. Notice also how we have to pinpoint the Right Hemisphere vertex data within ciiall.cdata data by using an offset because the ciiall.cdata is populated first with values from the left cortex but the Right Hemisphere starts indexing its vertices from 0. This is achieved already for you using the code findRHneighbors=findneighbors + ciiall.diminfo{1}.models{2}.start - 1 to identify the vertices and then plot(ciiall.cdata(findRHneighbors,:)') to display their fMRI series. An updated approach to the one mentioned above has now been provided as an optional section B . This section shows how the cifti-matlab helper function cifti_diminfo_dense_get_surface_info allows us to more efficiently identify the vertices on the RIGHT_CORTEX and map them to their corresponding data values in the ciiall.cdata matrix. This is the preferred approach to accessing data associated with surface structures in the cifti file. Please refer to the cifti-matlab library for additional convenience functions. In section C , we do the same for another arbitrary point on the Right cortex, vertex 8470 and we see a plot of the time series for this vertex and its neighbors. In section D , we now switch gears and look at a volume. We will investigate the Left Hippocampus which is indexed as model number 14 in the array of available models. Volumes are indexed using a voxel list voxlist which is part of the struct ciiall.diminfo{1}.models{14}.voxlist . ciiall.diminfo{1}.models{14} = struct with fields: start: 84561 count: 764 struct: 'HIPPOCAMPUS_LEFT' type: 'vox' voxlist: [3\u00d7764 double] The voxel list is actually a 2D matrix with 3 rows representing the i , j and k index coordinates in the x, y and z directions. ciiall.diminfo{1}.models{14}.voxlist(:,1:3)' ans = 55 60 21 56 60 21 57 60 21 We will now use an arbitrary voxel 56,56,25 within the Left Hippocampus and identify its potential neighbors. We allow diagonal voxel elements to be classified as neighbors and so a voxel can have potentially 26 close neighbors. Again as with the vertices we check the voxlist for existence as some of these voxels might be in other structures and thus not accessible within the Left Hippocampus. We do find that there are 26 neighbors to our voxel. Updates have been made to the code to use the function cifti_diminfo_dense_get_volume_structure_info in an optional section D to more efficiently identify and change the data associated with the LEFT_HIPPOCAMPUS. This is the preferred approach to accessing data associated with volume structures in the cifti file. In Section E we simulate an fMRI signal and place identical copies of it in vertex 17617 and its 6 neighbors, vertex 8470 and its 6 neighbours and voxel 56,56,25 and its 26 neighbors. And finally in section F we save a copy of this in a new CIFTI file called amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii Now let's return to wb_view . It might be easier to close all files and then load in the following files: From /DATA/HCP/100307/MNINonLinear/fsaverage_LR32k/ load in the following: 100307.R.midthickness.32k_fs_LR.surf.gii , 100307.L.midthickness.32k_fs_LR.surf.gii , and 100307.R.aparc.a2009s.32k_fs_LR.dlabel.nii From /DATA/HCP/100307/MNINonLinear/ load in the following: aparc.a2009s+aseg.nii.gz and T1w_restore_brain.nii.gz Finally we can now load in the amended fMRI from the current directory amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii Select Volume view and activate the amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii layer. Deselect the T1w_restore_brain.nii.gz layer and any other layer that may be activated in the Volume view. This ensures that the coodinate system of the amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii layer, takes precedence in the next step. In the Slice Indices/Coords section enter in the voxel coordinates of the left hippocampal voxel 56,56,25 in the first column. The cross-hairs should center on that voxel. Zoom in to the voxel by using the mouse scroll wheel. The image below shows an Axial volume view. Now activate the aparc.a2009s+aseg.nii.gz layer and click on the crosshairs to confirm voxel 56,56,25 is in the Left Hippocampus. Also notice that the voxels around it have the same value as we would expect. Deactivate the aparc.a2009s+aseg.nii.gz layer and now change the amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii layer. to dynconn - amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dynconn Now change the view to All , deactivate the Left cortex to make the left hippocampus more visisble. Activate the right cortex and rotate the brain to view the Right hemisphere and also activate the dynconn - amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dynconn layer. You should be able to identify that the vertices 8470 and 17617 are functionally connected to the hippocampus. This is what we expect as we placed identical fmri values in all these three regions. You may want to experiment with different color palette settings and/or views to get a more dramatic effect. Shown below is the magma color palette with a range defined from 0.2 to -0.2 used in the All view with the voxel settings set at 56,56,25 . Clicking on the right hemisphere outside of vertices 8470 or 17617 will reduce the functional connectivity of the hippocampal voxels. And clicking on those vertices will re-establish the functional connectivity with the hippocampus. Final words This concludes this introduction to the GIFTI and CIFTI formats. This practicum has used Matlab libraries to query, read and write Surface-Based formats however there are other approaches like the Python-based NiBabel library which provides functions for working with Cifti and Gifti files. To gain more experience using wb_view then material from from the HCP 2018 Course and the HCP's Connectome Workbench v1.0 Tutorial might be useful. Please provide any corrections and suggestions by email or as issues on GitHub Acknowledgements Huge thanks to Dianne Patterson for all her incredibly helpful comments and hard work testing this practicum. While all errors are mine, this exercise would have been much poorer without her support and insights. Thanks also to Tim Coalson for important clarifications on terminology, insight into the mex issue and also for pointing out efficient ways for using the cifti-matlab library. Troubleshooting wb_view prevented from running in Windows In windows 10 you may get a warning dialog stating that Windows protected your PC when you try to run wb_view.exe. If you downloaded this directly from the HCP website then you should be reassured that this is safe. Just click on More Info and then Run anyway . wb_view sluggish/behaving erratically in Windows It is possible that your system has an outstanding windows update. Run Windows Update on your computer. Shutdown and restart and then reinstall wb_view to fix this. Invalid MEX-file in Mac When running one or two of the exercises on the Mac you may get an error related to Invalid MEX-file . This is due to an incompatibility between the version of Matlab that you are running, the mac OS version and the compiled version of the mex file that is flagged as in error. It might be possible to solve this in one of three ways: Recompile the mex file in matlab to create a version that is compatible with you mac OS and matlab version Find a compatible mex file online that can act as a drop-in replacement for the problematic version. One good source for replacement mex files is the SPM library . If you are lucky then the mex file in error is one that only provides efficiency gains and is not critical to the functionality of the module. You might be able to simply delete the mex file to carry on with the practicum. References Glasser, M. F., Sotiropoulos, S. N., Wilson, J. A., Coalson, T. S., Fischl, B., Andersson, J. L., ... & Van Essen, D. C. (2013). The minimal preprocessing pipelines for the Human Connectome Project. Neuroimage, 80, 105-124. Glasser, M. F., Coalson, T. S., Robinson, E. C., Hacker, C. D., Harwell, J., Yacoub, E., ... & Smith, S. M. (2016). A multi-modal parcellation of human cerebral cortex. Nature, 536(7615), 171-178.","title":"practicum 8.1"},{"location":"surfdata/prac81/#practicum-81-surface-based-data-formats","text":"","title":"Practicum 8.1 -  Surface-Based Data Formats"},{"location":"surfdata/prac81/#synopsis","text":"Surface-based data formats represent neuroimaging data in two main forms namely meshes and overlays . In both cases anatomical locations are represented by vertices which are arbitrary points in 3D space. Surface meshes store 2 main types of information - the Cartesian location (x, y, z) of each vertex in 3D space and the triple of vertex indices that make up a triangular face on the mesh. Surface overlays store attributes for each vertex. These attributes could be single value scalars like the cortical thickness at a vertex or a series of values like a time series from an fMRI or EEG experiment.","title":"Synopsis"},{"location":"surfdata/prac81/#objectives","text":"This practicum will introduce you to working with surface-based data (meshes and overlays) using the GIFTI and CIFTI file formats. By the end of this practicum you should Be able to access the metadata of GIFTI and CIFTI files to determine the properties of the surface under study Be able to directly access the image data in a GIFTI and CIFTI file and work with it Be able to change the data in a GIFTI and CIFTI file and create a new file with your changes Visualize your GIFTI and CIFTI files using the workbench tool Note Some of the tasks you will accomplish with this practicum will not ordinarily be part of your typical neuroimaging analysis workflow. For example you will rarely if ever need to change the shape of a surface mesh. However the intention of these exercises is to cement your understanding of these formats by giving you hands-on experience with manipulating them albeit in somewhat artificial scenarios.","title":"Objectives"},{"location":"surfdata/prac81/#downloadsinstallation","text":"This practicum requires the following materials/folder setup for successful complete: A reasonably recent version of matlab . The practicum was tested on version R2020a (linux), R2019a (windows) and R2018b (Mac) but should hopefully work on older versions as well. A copy of the HCP's Connectome workbench . We will be using wb_view for viewing CIFTI and GIFTI files. The install is a zip file that you extract and the executable should be available after extraction. Depending on your platform you may need to double-click the wb_view icon or add it to your path to access it via the command line. Copies of workbench have also been packaged up at the OSF site for this lesson under the 08-Surface-Based-Formats/Software folder which you can access if you have trouble getting to the connectome site. There are 4 zip files with names that start with workbench-.. for different OS platforms. The Matlab practicum files, Matlab libraries and data for this practicum are also all available at the OSF site folder under the 08-Surface-Based-Formats/practicum81 folder in a zip file practicum81.zip . Download this zip file to your local computer and extract. Your folder structure should look like this after extraction. \u2514\u2500\u2500 practicum81 \u251c\u2500\u2500 DATA \u2502 \u2514\u2500\u2500 HCP \u251c\u2500\u2500 matlab-library \u2502 \u251c\u2500\u2500 cifti-matlab \u2502 \u251c\u2500\u2500 gifti-release \u2502 \u251c\u2500\u2500 helper-functions \u2502 \u2514\u2500\u2500 xml2struct \u251c\u2500\u2500 task_002_giftisurf.m \u251c\u2500\u2500 task_003_giftioverlay.m \u251c\u2500\u2500 task_004_cifti.m \u2514\u2500\u2500 task_005_cifti.m","title":"Downloads/Installation"},{"location":"surfdata/prac81/#provenance","text":"","title":"Provenance"},{"location":"surfdata/prac81/#matlab-libraries","text":"The Matlab libraries used in this practicum are described below: The HCP cifti-matlab library was in alpha-testing phase at the time of creating this practicum and so is liable to change in the future though hopefully not significantly. The version provided had its last commit 1e0ca9fbb0f9a8105d144ec6bc0de55b84d1b528 on Wed Sep 2 15:42:55 2020 -0500 . Please refer to the cifti-matlab repository for announcements on when the alpha-testing is complete. In the meantime if you use this code for your own research then please appreciate that you do this at your own risk and there is a chance of errors. [ Update on 11/2/2020, the cifti-matlab library was released as a beta version 2.0.0 for public testing on 9/30/2020. Comments from Tim Coalson summarizing the benefits of this version are available here ] The gifti library was downloaded as a zip file gifti-master.zip , extracted and renamed to gifti-release xml2struct was downloaded directly from the mathworks file-exchange. helper-functions mostly contains custom functions written in-house to support tasks 4 and 5. The function read_nifti2_hdr.m is a private function from the FieldTrip toolbox to help with reading NIFTI-2 files.","title":"Matlab libraries"},{"location":"surfdata/prac81/#data","text":"We will be using a subset of data from the Human Connectome Project's Young Adult (YA) 1200 Subject Release . Specifically data from subject 100307. The HCP has terms of reference for using their data. To use HCP data please sign on to the HCP website and register for free.","title":"Data"},{"location":"surfdata/prac81/#miscellaneous-files","text":"The documentation for this practicum has also been bundled with the practicum materials as prac81.pdf to mitigate against internet connectivity problems. The other four matlab files task_002_giftisurf.m , task_003_giftioverlay.m , task_004_cifti.m and task_005_cifti.m are needed to undertake the respective tasks below.","title":"Miscellaneous files"},{"location":"surfdata/prac81/#task-1-use-wb_view-to-visualize-gifti-surfaces","text":"Open wb_view either by double-clicking the wb_view or wb_view.exe icon in your windows manager or by entering this in the command line if you have set up your paths in the command line interface correctly. When wb_view opens click the skip button in the dialog to get to the main screen From the top menu choose File > Open File and change the dropdown on Files of type: to Surface Files (*.surf.gii) Now navigate to practicum81/DATA/HCP/100307/MNINonLinear/Native/ You will be presented with a host of files with the left mouse button click on 100307.L.pial.native.surf.gii Now pressing the CTRL key (Command key on Mac) again click with the left mouse button on 100307.R.pial.native.surf.gii to select both surfaces. You should see that both files are reflected in the File name textbox. The pial that these surfaces refer to is the surface boundary between grey matter and CSF in the outer cortex. Now click Open , you should see the left and right pial surfaces displayed with lateral and medial views Information In several exercises we make use of data on the subject's native mesh as in the example above with the 100307.L.pial.native.surf.gii and 100307.R.pial.native.surf.gii surfaces. It's important to mention that most of the HCP data is however not on the native mesh but on a standard fs_LR mesh on which the left and right hemispheres of all subjects have been aligned using the MSM-All surface-based registration approach Glasser et al, 2016 . There are two important standard spaces used in the HCP, namely, 164k_fs_LR which is a high resolution space of 163,842 vertices per hemisphere (at approx 1mm resolution) and the 32k_fs_LR space which is a lower resolution space of 32492 vertices per hemishere (at approx 2mm resolution). We will use an example of the latter in task 5. For more details about these standard meshes consult Glasser et al, 2013 and the HCP Wiki Page At the top of the application change from (1) Montage to (2) All by clicking on the tab. You should see the left and right pial surfaces aligned on the screen. Use your mouse left button to rotate the brain around and your scroll wheel to zoom in and out We can get a quick overview of these surfaces by choosing Surface > Information . We can see that there are 130879 vertices and 261754 faces in the left hemisphere pial surface. For the rest of this exercise we just want to work with left hemisphere so we will uncheck the display for the right cortex in the (2) All tab. Switch on edge view by clicking the Surface menu option. Choose Properties and change the Drawing Type to Links (Edges) . Zoom in a little closer to see the triangular tesselation. identify any vertex you like and click on it. For this example I have stumbled upon vertex 119115 which I have decided to probe. A dialog with title Information should pop up with vertex information. This dialog provides information about the vertex's hemisphere, label and its anatomical location in 3D Cartesian (x,y,z) coordinates. Notice also that the white vertex symbol icon is much larger than the width of the edges. Choose Properties in the information dialog ( Properties is under Contra ID in the dialog) and change the Symbol Diameter and Most Recent ID Symbol Diameter to a suitable size. We chose 0.5mm for our example. You can also change the ID Symbol Color to another value. Once you have selected a vertex we can now try and identify its neighbors. For the first pass through this practicum you should avoid choosing a vertex on the medial wall as this will cause difficulties in task 4. To be super safe then just replicate the vertex we have chosen or choose a vertex that is easilly accessible on the lateral surface. The medial wall contains vertices that are not actually on the cortical surface (actually these medial-wall vertices cut through subcortical grey matter structures and the Corpus callosum) but are required to maintain the spherical topology of the surface. Select all the neighbouring vertices (all vertices connected by an edge to your chosen vertex). These should show up in the information screen. You can select and copy the text from this screen and place it in a text file. We will need this information for the next Task. You can leave wb_view open but minimized and proceed to the next task.","title":"Task 1: Use wb_view to visualize GIFTI surfaces"},{"location":"surfdata/prac81/#task-2-use-matlab-to-manipulate-gifti-surfaces","text":"In this task we will be loading in the GIFTI surface 100307.L.pial.native.surf.gii which we visualized above and identifying the vertex and vertex neighbours we chose. We will then change the local shape of the surface by nudging these vertices by a random 3D translation and then save a new copy of the surface which we will visualize again using wb_view . Open Matlab and navigate to your the practicum81 folder containing task_002_giftisurf.m . Double click this file to open it in Matlab. We will run individual sections of the Matlab code by right-clicking on each section (highlighted yellow when selected) and selecting Evaluate Current Section . Select the 1st section and choose Evaluate Current Selection to run. This will ensure that Matlab can find the gifti-release library which we need to open and save GIFTI files. If you haven't set the right path to the gifti-release library then you will see an error like this: Warning: Name is nonexistent or not a directory /home/chidi/Downloads/ Practicum81/../matlab-library/gifti-release Now right-click and run Section A . If the variable mysurf is not pointing to the correct location for the data then you will get an error similar to this: Reading Surface ../DATA/HCP/100307/MNINonLinear/Native/100307.L.pial.native.surf.gii Error using xml_parser Cannot read XML document This section will plot a 3D model of the surface which can be rotated with the mouse. This is just to demonstrate that you can visualize your surface mesh within Matlab as well. You can close these windows whenever you want. you should also see that the expected number of faces and vertices are also printed to the screen. This information has been obtained using the gifti-release libraries gifti() function. The information is stored in the structure mysurfleft_mesh mysurfleft_mesh = struct with fields: faces: [261754\u00d73 int32] mat: [4\u00d74 double] vertices: [130879\u00d73 single] Note One source of potential confusion is that the gifti Matlab library indexes vertices starting from 1 but wb_view and the GIFTI specification start indexing from 0. So the 1st vertex has an index of 0 in wb_view but has an index value of 1 in Matlab . The faces matrix contains the 3 vertex indices that are joined together to form a face on each row. In Matlab we can view the first 4 faces as follows: mysurfleft_mesh.faces(1:4,:) 1 2 3 4 3 2 1 56 2 57 2 56 We see that the first face (row 1 above) comprises vertices 1,2 and 3 which are joined together by edges. Perhaps surprisingly we see that 56 and 57 are also joined to 2 to form the fourth face. This reinforces the fact that the vertex index is just a label and does not predict spatial proximity. Remember that this uses Matlab's indexing conventions. So wb_view would specify the first face as consisting of vertices 0,1 and 2. The vertices matrix provides the anatomical location of each vertex. Looking at the first 4 vertices, we see that vertex 3 (i.e. vertex 2 in wb_view convention) has a location of -13.3332 mm, -106.5394 mm, -5.5976 mm mysurfleft_mesh.vertices(1:4,:) -13.0844 -106.5673 -5.1778 -13.5373 -106.6587 -5.1817 -13.3332 -106.5394 -5.5976 -13.7358 -106.5008 -5.7497 Now run the next section B to find the neighbours of the vertex you located in task 1 above. Remember to change the vertex on line 25 ciftivind= to point to your vertex. If all goes well then you should see the same vertices you observed in wb_view . As already mentioned wb_view (and the GIFTI XML) start indexing vertices from 0 while the gifti matlab library starts indexing from 1. So we need to add 1 to the index we obtained from wb_view to create the correct index for use in Matlab. This is already accomplished for you in the code on line 29 by matlabvind=ciftivind + 1; The code looks at all the columns of the faces matrix and picks out the lines that include our vertex and then does a union to create a set of vertices without duplication. The faces matrix contains a row for every triangular face. So below for example, the vertices 119116, 119106 and 119115 are connected as a face. Find 119116 in column 1: 119116 119106 119115 119116 119128 119129 119116 119129 119117 Find 119116 in column 2: 118448 119116 119115 119106 119116 119107 Find 119116 in column 3: 118448 118449 119116 118449 119128 119116 119117 119107 119116 Amalgamate all the vertices and remove duplicates: 118448 118449 119106 119107 119115 119117 119128 119129 Now we will run section C to perturb each neighbouring vertex by a random number between 2 and -2 mm. Note: This scenario will rarely happen in real-world analysis but has been designed to give you insight into the internal workings of surface-based formats. Packages like FreeSurfer do allow you to manually manipulate surfaces to correct for errors in initial modelling and this exercise demonstrates how this would work in principle. This code goes through a loop and for each vertex obtains the x, y and z anatomical location information and perturbs it by a random number. We then create a new GIFTI file called amended.100307.L.pial.native.surf.gii in your current folder which contains the changed vertex locations in a new surface. Now we will return to wb_view to view your new GIFTI file. Open your amended GIFTI surface amended.100307.L.pial.native.surf.gii that was created in the Practicum81 folder in wb_view The original 100307.L.pial.native.surf.gii that was obtained from the folder Practicum81/DATA/HCP/100307/MNINonLinear/Native/ should already be loaded into wb_view if it was not closed previously. If not then open this GIFTI file as well. We will be comparing both of them in wb_view. By default there are two tabs provided when you first open wb_view these are (1) Montage and (2) All You can click on any of the tabs and change the View to Surface by clicking the corresponding radio button. It's also possible to create a new tab and change the view to Surface there. Ensure that under Brain Structure and Surface that the first drop down box is set as All or as CortexLeft and that the second drop down is set to our amended.100307.L.pial.native.surf.gii as the selected surface. let's locate our original vertex from task 1. The sequence of steps to take will depend on the state your GUI is in based on subsequent actions. The steps below should hopefully get you there regardless. Click on Window and then identify and click on the identify Surface Vertex and enter the vertex Index. For our example we used 119115 . If you have several vertices displayed which are confusing you can clear all vertices by clicking on RID button on the Information Properties dialog. After clicking apply it might also help to change the size and color of the vertex symbol to a larger value and/or more vivid color respectively if you haven't already done so previously. You may also have to zoom out and/or rotate the brain if you chose a vertex hidden from the lateral view. You should be able to spot your vertex. Now Change the Surface properties to Link(Edges) (if not already changed from previous steps) and zoom in closer to see how the pertubations you made to the neighboring vertices have changed the shape of the mesh. You will probably need to change the symbol diameter to something smaller. You should notice that the spatial location of the vertices has changed. You can contrast this with the original by clicking back and forth on the dropdown in the tab (under Brain Structure and Surface ) switching between the original surface and your amended surface. You can also click on a Montage view to see both surfaces side by side (You will need to deselect either the medial or lateral checkbox to see the vertex and neighbours properly side by side). You will also need to select both surfaces as different layers in the montage selection. You should confirm that you have successfully changed the shape of your surface. We won't dwell too long on the next two sections but these sections give you more insight into the XML structure of the GIFTI structure so that you can compare that with the data provided by the gifti matlab library. Essentially this library just reads this XML in the GIFTI to create the faces and vertices structures that you have just looked at. We will return to the Matlab file to run section D to view the GIFTI metadata in a browser to see exactly how the XML is used to represent the GIFTI format. All this section does is re-save the GIFTI in ASCII format as mygifti_allxml.gii so that most text editors can view it conveniently. After running section D, you can now simply open the created mygifti_allxml.gii file directly in your browser or by using VScode if it does not open automatically in firefox. (you may get an error on windows that states 'firefox is not recognized..' - this can be completely ignored and the command console that opens with this message can be closed) Some browsers like Chrome may require you to explicitly rename this as an xml file mygifti_allxml.xml in order to view the XML with helpful highlighting and the ability to collapse and expand sections. You can collapse and expand XML sections in the browser by clicking on the -/+ signs respectively. Notice that the vertex anatomical location information is represented as a DataArray with an Intent name of NIFTI_INTENT_POINTSET . The number of vertices is captured by Dim0=\"130879\" . The triangular faces are represented in the second data array as NIFTI_INTENT_TRIANGLE . Expand the Data tag in the NIFTI_INTENT_POINTSET dataarray and then look at the surface mesh vertex information that is loaded in by the gifti library. Notice how the columns are loaded in first before the rows. This is known as Column Major Order and is specified also in the XML as a DataArray attribute as <DataArray ArrayIndexingOrder=\"ColumnMajorOrder\".. Notice also that for the faces defined as NIFTI_INTENT_TRIANGLE in the second DataArray that the vertices are indexed starting from zero and are also stored using Column Major Order. Section E accomplishes the same as Section D except that now the xml data is loaded into a Matlab structure. You can skip this section as all it does is corroborate what you have already seen above. It is divided into 2 parts, E1 and E2 Run Section E1 to fix the xml in the GIFTI. We need to do this as unfortunately there is a slight problem reading the XML file into Matlab as the 2nd line which starts <!DOCTYPE causes problems with the parser. The line sed 2d... in the Matlab file is written to delete this line on unix and save a new text file called mygifti_xml.gii which is then parsed. In windows you will get an error that states 'sed is not recognized..' - this can be completely ignored and the command console that opens with this message can be closed). You will need to use powershell instead if its available or alternatively manually open the xml file in a text processor and delete the 2nd line. In Matlab you can invoke a powershell shell as follows: !powershell After powershell loads enter the command below to remove the 2nd line: Get-Content mygifti_allxml.gii | Where {$_ -notmatch 'DOCTYPE'} | Set-Content mygifti_xml.gii Then return to Matlab by typing exit Now run Section E2 to load the XML into a Matlab structure. You can now identify the same XML structures as we saw above in the browser. For example myXML.GIFTI.DataArray{2}.Data gives us the faces information for the surface: myXML.GIFTI.DataArray{2}.Data = Text: '0 3 0 56 0 0 1 1 2 2 2 72 3 3 4 4 6 14 6 6 6 11","title":"Task 2: Use Matlab to manipulate GIFTI Surfaces"},{"location":"surfdata/prac81/#task-3-use-matlab-to-manipulate-and-visualize-gifti-overlays","text":"We have been able to work with and manipulate GIFTI surface files . In this task we will now look at an example of a GIFTI overlay that maps cortical thickness values onto the pial vertices and we will attempt to manipulate and visualize this overlay. If wb_view is already open from previous tasks then you can use the File > Save/Manage Files functionality to selectively close files. Select to close the Right Pial and the amended Left pial. We will be using the original Left pial in this task. Alternatively if you want to start from scratch then just choose File > Close all files in wb_view load in the surface 100307.L.pial.native.surf.gii again from Practicum81/DATA/HCP/100307/MNINonLinear/Native/ if not already open. Click on Surface to view the Left hemisphere. Now load in the cortical thickness overlay 100307.L.thickness.native.shape.gii from the same location. You will need to change the Files of type field to Metric Files (*.func.gii *.shape.gii) or Any File (*) to be able to select the thickness overlay. Activate the layer by clicking on the checkbox alongside it. Also click on the color bar to see the range of cortical thicknesses across the brain and their associated colors. Now locate the vertices you looked at previously (using Window > identify ) and find their cortical thicknesses by looking at the information dialog. Notice that for our central vertex (119115 in this case) we have a cortical thickness of 3.54125 mm. We will now open this overlay in Matlab, change the cortical thickness of our target vertex and its neighbors and visualize the changed overlay in wb_view Open the file task_003_giftioverlay.m in matlab. As before run each section in Matlab separately to follow along with what is happening in the code. First run the addpath .. section to ensure that the gifti library is accessible and that all Matlab windows are closed and variables cleared. In section A we load in the surface as before and now also load in the cortical thickness overlay. Notice that the overlay and the surface match as the number of vertices is equal to the number of thickness values. Reading Surface ./DATA/HCP/100307/MNINonLinear/Native/100307.L.pial.native.surf.gii Surface has 261754 faces Surface has 130879 vertices Reading Overlay ./DATA/HCP/100307/MNINonLinear/Native/100307.L.thickness.native.shape.gii Overlay has 130879 values The overlay has all its thickness values for each vertex stored in a cdata matrix: myoverlayleft = struct with fields: cdata: [130879\u00d71 single] Using the gifti library's overloaded plot command we also visualize the surface in one window and also observe overlay plotted on the surface in another window with a color scale used to designate varying thickness. In Section B we see a print out of the cortical thickness values at our neighbouring vertices which should match the values we identifed in wb_view . In Section C we set the thicknesses of our chosen vertex and its neighbors to a uniform value of 2mm and then save a copy called amended.100307.L.thickness.native.shape.gii . Return to wb_view and open the amended.100307.L.thickness.native.shape.gii overlay. Compare this with the original 100307.L.thickness.native.shape.gii overlay and confirm that all the vertices now have a thickness value of 2mm. You can change what overlays are activated and the precedence in which they are displayed by selecting different files and/or toggling the visibility of the layer. You may also want to change the Drawing Type in Surface > Properties to Triangles to see a clear thickness change in the patch of cortex as you toggle between layers. In addition you may also want to view the amended cortical thickness overlay amended.100307.L.thickness.native.shape.gii on your amended pial surface amended.100307.L.pial.native.surf.gii or on other surfaces that are available in the same directory. These surfaces are all in vertex correspondence for the subject 100307. In section D and E you can again study the XML structure of the overlay as you did in task 2 above to see how it corresponds to the gifti structure loaded into Matlab.","title":"Task 3: Use Matlab to manipulate and visualize GIFTI overlays"},{"location":"surfdata/prac81/#task-4-use-matlab-to-manipulate-and-visualize-cifti-files","text":"So far we have learned to work with GIFTI files. In this penultimate task we are going to work with CIFTI files which combine both volume data for subcortical structures and surface data for cortical structures into one file. Unfortunately the HCP's Matlab library was still in Alpha testing at the time of creating this practicum and so we need to be aware that there is a slight chance that the coding conventions used here may change however the overal conceptual approach should remain the same. We will work with HCP data again and this time with CIFTI cortical thickness data 100307.thickness.native.dscalar.nii from the left and right cortex. To visualize CIFTI data in wb_view we need a GIFTI surface file that is associated with the CIFTI data. Close all loaded surfaces and overlays if wb_view is already open. Open wb_view and load in GIFTI surface 100307.L.midthickness.native.surf.gii - this surface is midway between the pial surface we used previously and the white surface. Now load in the midthickness surface for the right hemisphere 100307.R.midthickness.native.surf.gii . Click on All view to see both hemispheres aligned in the view. You can rotate the whole brain to a perspective that works for you. Now open 100307.thickness.native.dscalar.nii - you will need to specify Files of type: (*.dscalar.nii) to be able to select this file. Activate the layer by clicking on the checkbox and notice that this CIFTI file has cortical thickness information for both hemispheres. lets return to Matlab and open the Matlab file task004_cifti.m and execute the prelim section to load in the Matlab libraries that we need. In Section A we will briefly explore the overlay 100307.thickness.native.dscalar.nii as a NIFTI-2 file. We will use a private function read_nifti2_hdr.m which is packaged as part of the FieldTrip Matlab library. The diagram below shows how a NIFTI-2 file is used to package up this CIFTI. The extra space used in the NIFTI-2 file to store the cifti-XML is called a cifti extension or extension . We can confirm that the size of the CIFTI file in bytes is equal to the size of the voxel offset field plus the data size in bytes: >> filesize = 2525056 >> nii2.vox_offset 1531104 >> nii2.dim 6 1 1 1 1 1 248488 1 where the value of nii2.dim[7] = 248488 is the data size in floats which is equal to 4*248488=993952 in bytes. The XML is thus embedded in the extent section between the header and the data. It is this section that the cifti-matlab library reads to determine exactly how the surface and volume models are set up. Run Section B to explore the CIFTI XML file in a Matlab structure and in a browser. Notice that like the gifti overlay this cifti overlay has data for both cortices in a cdata structure which can be accessed as ciiall.cdata >> ciiall ciiall = struct with fields: metadata: [1\u00d73 struct] diminfo: {[1\u00d71 struct] [1\u00d71 struct]} cdata: [248488\u00d71 single] The xml data has also been stored in a CIFTI structure >> ciixml ciixml = struct with fields: CIFTI: [1\u00d71 struct] The information stored in the above two structures matches the XML data that can be visualized in the browser. The code has been set up to open the CIFTI XML automatically in firefox if it is installed in your path but this may not work on your system without some tweaking. You can also simply view the XML directly in a browser or in VScode by opening the file debug.xml that is created in your current folder. Notice that the first dimension is CIFTI_INDEX_TYPE_SCALARS which means that each vertex/voxel has a single scalar value which in this file is the cortical thickness. Also notice that the second dimension is CIFTI_INDEX_TYPE_BRAIN_MODELS which is used to map every voxel/vertex in a cortical or subcortical structure. Just the left cortex CIFTI_STRUCTURE_CORTEX_LEFT and the right cortex CIFTI_STRUCTURE_CORTEX_RIGHT are represented in this file. Information for each cortex can be obtained by looking at the diminfo fields. Here we look at the model for the CORTEX_LEFT which is contained in diminfo{1}.models{1} >> ciiall.diminfo{1}.models{1} ans = struct with fields: start: 1 count: 125197 struct: 'CORTEX_LEFT' type: 'surf' numvert: 130879 vertlist: [1\u00d7125197 double] Unlike the GIFTI Matlab library that indexed vertices starting from 1, this CIFTI library uses an index to index the vertices! This is a little complicated but in essence if we look at the vertlist structure for the first three vertices in the left hemisphere we see that they are correctly identified as 0,1 and 2. ciiall.diminfo{1}.models{1}.vertlist(1:3) ans = 0 1 2 As already mentioned, the data structure ciiall contains all the cortical thickness data for both hemispheres in ciiall.cdata indexed starting from 1 to 125197 for the left cortex and from 125198 to 248488. These indexes are used as pointers into a vertex list vertlist which is stored for each surface. So the cortical thickness values for the first three vertices 0,1,2 in the Left hemisphere are obtained as follows: ciiall.cdata(1:3) ans = 3\u00d71 single column vector 2.201649 2.323979 2.41378 There however appears to be an inconsistency with the total number of vertices allocated to the overlay. In the last task we had 130879 vertices in the left hemisphere but here we have 125197 vertices assigned (roughly about 5600 vertices fewer). What's going on? Essentially the HCP CIFTI files do not store data for the medial wall of each cortex. We saw the medial wall in the first task. We will identify these medial vertices and visualize a few of them on the left cortex. Run section C to print out all the vertices that are missing from the left cortex. These are the medial vertices. There should be 5682 vertices in the medial wall displayed to screen. Select one or a few of them and using the identify tool in wb_view try to see where they are. You will need to use the surface view and visualize the left cortex only so you can see these vertices. Some of these are easier to spot than others. In section D and E we do what we did in task 3 and will now go ahead and change the value of a vertex and its neighbors to a constant value of 2mm cortical thickness. The code uses a vertex value of 119115 but of course you should use any vertex you like except for the medial vertices which the CIFTI file does not track. Open the amended.100307.thickness.native.dscalar.nii that is created in your current directory in wb_view and navigate to the vertex to confirm that changes have been made. An additional section at the end of the file provides code to replace sections E and F, which shows how the cifti-matlab helper function cifti_diminfo_dense_get_surface_info allows us to more efficiently identify the vertices on the LEFT_CORTEX and map them to their corresponding data values in the ciiall.cdata matrix. This is the preferred approach to accessing data associated with surface structures in the cifti file. The equivalent function for accessing sub-cortical structures is cifti_diminfo_dense_get_volume_structure_info . Please refer to the cifti-matlab library for additional convenience functions.","title":"Task 4: Use Matlab to manipulate and visualize CIFTI files"},{"location":"surfdata/prac81/#task-5-use-matlab-to-manipulate-and-visualize-cifti-files-with-volumes-and-surfaces-combined","text":"We now look at a slightly more complex CIFTI file which stores fMRI data for Surface and Volume structures simultaneously. We will be working in the HCP's standard space for this and so will close all the files in wb_view and then load in the surfaces which we will be using for this exploration. Click on File > Open File and navigate to /DATA/HCP/100307/MNINonLinear/fsaverage_LR32k/ and open the surfaces 100307.R.midthickness.32k_fs_LR.surf.gii and 100307.L.midthickness.32k_fs_LR.surf.gii select the All view and then load in the fMRI dtseries.nii CIFTI file rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii from the location DATA/HCP/100307/MNINonLinear/Results/rfMRI_REST1_LR/ . Change the file overlay from dynconn - rfMRI_REST1_LR_Atlas_hp2000_clean.dynconn to the dtseries rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii and switch on the overlay to see the first volume of the resting state fMRI series which has an index of 1 and a Map value of 0 seconds. Rotate the brain to get a better view of the fMRI data. Notice that there are values in both the cortex (surface-based data represented by vertices) and in the subcortex (volume-based data represented by voxels). To navigate the volume-based data more conveniently you can click on the Volume view to access the traditional volume viewing interface. In Slice Plane pane (not the View Pane!) you can select All to view Axial, Parasagittal and Coronal planes at the same time. Again you will need to switch on the overlay and choose rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii over the dynconn... overlay. You can move through volume slices by holding the left mouse button and dragging the mouse. You can zoom in by using the mouse scroll wheel. To help with spatial awareness you can load in a NIFTI-1 T1w volume. Open T1w_restore_brain.nii.gz which is available at /DATA/HCP/100307/MNINonLinear . Activate the overlay so that it is visible. In both the voxel and surface view, individual fmri instances at each TR can be stepped through by clicking on the index arrows. You can also navigate to a specific timepoint in the series by clicking on the map drop down and selecting the time instance of interest. One nice application within wb_view is the ability to interactively view functional connectivity. To see this in action Let us activate the All tab. We can then load in Freesurfer's Destrieux atlas which has been converted to gifti format. The left and right hemisphere atlases are called 100307.L.aparc.a2009s.32k_fs_LR.label.gii and 100307.R.aparc.a2009s.32k_fs_LR.label.gii and are available here /DATA/HCP/100307/MNINonLinear/fsaverage_LR32k/ . Activate all three overlays in the All view and from top to bottom select the Dynconn - rfMRI.. layer, then the Left and Right atlas label files. Now click on any brain area and you will see the functional connectivity of every other vertex to that vertex which has been calculated dynamically. If you spin the cortex around you will also see that the connectivity is also calculated for voxels as well. The areas which are functionally connected are displayed with colors that represent higher correlation. So in the example below all the yellow areas are functionally connected. For the above we used a different color map which is accessible from the little wrench symbol. You can also view the scale used for the color bar by clicking the multi-colored bar. Back to Matlab now so we can explore a little more how this CIFTI file stores both volumes and surfaces in the same NIFTI-2 file. Open task_005_cifti.m and run the first section as usual to clear the workspace and add the necessary libraries to our path. Run Section A to see how the CIFTI XML has different sections for surfaces and for voxels. Note that each volume structure has an individual label e.g CIFTI_STRUCTURE_ACCUMBENS_LEFT. As you did in Task 4 you can query the XML directly in the structure ciixml and also compare this with the data that has been stored in the data structure ciiall by the HCP Matlab library. The output that is displayed in the command window is obtained by going through the XML file. It shows that information about voxels and vertices are stored at the same time in the XML file and that the actual overlay data for each vertex and voxel is indexed sequentially in the NIFTI-2 file. So for example with our CIFTI file if we read off a value at row position 59695 in the data array then this will be a value in AMYGDALA_LEFT (between 59688 and 60002) ... Model 1 is a surface of CORTEX_LEFT with 29696 verts valued out of 32492 vertices; start from 1 and end at 29696 Model 2 is a surface of CORTEX_RIGHT with 29716 verts valued out of 32492 vertices; start from 29697 and end at 59412 Model 3 is a volume of ACCUMBENS_LEFT with 135 voxels; start from 59413 and end at 59547 Model 4 is a volume of ACCUMBENS_RIGHT with 140 voxels; start from 59548 and end at 59687 Model 5 is a volume of AMYGDALA_LEFT with 315 voxels; start from 59688 and end at 60002 Model 6 is a volume of AMYGDALA_RIGHT with 332 voxels; start from 60003 and end at 60334 ... The code has been set up to open the CIFTI XML automatically in firefox if it is installed in your path but this may not work on your system without some tweaking. You can also simply view the XML directly in a browser or in VScode by opening the file debug.xml that is created in your current folder. In section B , we will now take an arbitrary vertex 17617 in the Right hemisphere and plot the fmri data in that vertex along with its closest neighbors. Observe that we have to check for existence of the neighbors that we find in the available list of vertices because one of the neighbors might actually be in the medial wall. This is achieved for you using the code neighindex=ismember(visverts,allverts); which takes allverts which includes vertex 17617 as well as its close neighbours and compares it against all the visible vertices that are not on the medial wall. Notice also how we have to pinpoint the Right Hemisphere vertex data within ciiall.cdata data by using an offset because the ciiall.cdata is populated first with values from the left cortex but the Right Hemisphere starts indexing its vertices from 0. This is achieved already for you using the code findRHneighbors=findneighbors + ciiall.diminfo{1}.models{2}.start - 1 to identify the vertices and then plot(ciiall.cdata(findRHneighbors,:)') to display their fMRI series. An updated approach to the one mentioned above has now been provided as an optional section B . This section shows how the cifti-matlab helper function cifti_diminfo_dense_get_surface_info allows us to more efficiently identify the vertices on the RIGHT_CORTEX and map them to their corresponding data values in the ciiall.cdata matrix. This is the preferred approach to accessing data associated with surface structures in the cifti file. Please refer to the cifti-matlab library for additional convenience functions. In section C , we do the same for another arbitrary point on the Right cortex, vertex 8470 and we see a plot of the time series for this vertex and its neighbors. In section D , we now switch gears and look at a volume. We will investigate the Left Hippocampus which is indexed as model number 14 in the array of available models. Volumes are indexed using a voxel list voxlist which is part of the struct ciiall.diminfo{1}.models{14}.voxlist . ciiall.diminfo{1}.models{14} = struct with fields: start: 84561 count: 764 struct: 'HIPPOCAMPUS_LEFT' type: 'vox' voxlist: [3\u00d7764 double] The voxel list is actually a 2D matrix with 3 rows representing the i , j and k index coordinates in the x, y and z directions. ciiall.diminfo{1}.models{14}.voxlist(:,1:3)' ans = 55 60 21 56 60 21 57 60 21 We will now use an arbitrary voxel 56,56,25 within the Left Hippocampus and identify its potential neighbors. We allow diagonal voxel elements to be classified as neighbors and so a voxel can have potentially 26 close neighbors. Again as with the vertices we check the voxlist for existence as some of these voxels might be in other structures and thus not accessible within the Left Hippocampus. We do find that there are 26 neighbors to our voxel. Updates have been made to the code to use the function cifti_diminfo_dense_get_volume_structure_info in an optional section D to more efficiently identify and change the data associated with the LEFT_HIPPOCAMPUS. This is the preferred approach to accessing data associated with volume structures in the cifti file. In Section E we simulate an fMRI signal and place identical copies of it in vertex 17617 and its 6 neighbors, vertex 8470 and its 6 neighbours and voxel 56,56,25 and its 26 neighbors. And finally in section F we save a copy of this in a new CIFTI file called amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii Now let's return to wb_view . It might be easier to close all files and then load in the following files: From /DATA/HCP/100307/MNINonLinear/fsaverage_LR32k/ load in the following: 100307.R.midthickness.32k_fs_LR.surf.gii , 100307.L.midthickness.32k_fs_LR.surf.gii , and 100307.R.aparc.a2009s.32k_fs_LR.dlabel.nii From /DATA/HCP/100307/MNINonLinear/ load in the following: aparc.a2009s+aseg.nii.gz and T1w_restore_brain.nii.gz Finally we can now load in the amended fMRI from the current directory amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii Select Volume view and activate the amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii layer. Deselect the T1w_restore_brain.nii.gz layer and any other layer that may be activated in the Volume view. This ensures that the coodinate system of the amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii layer, takes precedence in the next step. In the Slice Indices/Coords section enter in the voxel coordinates of the left hippocampal voxel 56,56,25 in the first column. The cross-hairs should center on that voxel. Zoom in to the voxel by using the mouse scroll wheel. The image below shows an Axial volume view. Now activate the aparc.a2009s+aseg.nii.gz layer and click on the crosshairs to confirm voxel 56,56,25 is in the Left Hippocampus. Also notice that the voxels around it have the same value as we would expect. Deactivate the aparc.a2009s+aseg.nii.gz layer and now change the amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dtseries.nii layer. to dynconn - amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dynconn Now change the view to All , deactivate the Left cortex to make the left hippocampus more visisble. Activate the right cortex and rotate the brain to view the Right hemisphere and also activate the dynconn - amended.rfMRI_REST1_LR_Atlas_hp2000_clean.dynconn layer. You should be able to identify that the vertices 8470 and 17617 are functionally connected to the hippocampus. This is what we expect as we placed identical fmri values in all these three regions. You may want to experiment with different color palette settings and/or views to get a more dramatic effect. Shown below is the magma color palette with a range defined from 0.2 to -0.2 used in the All view with the voxel settings set at 56,56,25 . Clicking on the right hemisphere outside of vertices 8470 or 17617 will reduce the functional connectivity of the hippocampal voxels. And clicking on those vertices will re-establish the functional connectivity with the hippocampus.","title":"Task 5: Use Matlab to manipulate and visualize CIFTI files with volumes and surfaces combined"},{"location":"surfdata/prac81/#final-words","text":"This concludes this introduction to the GIFTI and CIFTI formats. This practicum has used Matlab libraries to query, read and write Surface-Based formats however there are other approaches like the Python-based NiBabel library which provides functions for working with Cifti and Gifti files. To gain more experience using wb_view then material from from the HCP 2018 Course and the HCP's Connectome Workbench v1.0 Tutorial might be useful. Please provide any corrections and suggestions by email or as issues on GitHub","title":"Final words"},{"location":"surfdata/prac81/#acknowledgements","text":"Huge thanks to Dianne Patterson for all her incredibly helpful comments and hard work testing this practicum. While all errors are mine, this exercise would have been much poorer without her support and insights. Thanks also to Tim Coalson for important clarifications on terminology, insight into the mex issue and also for pointing out efficient ways for using the cifti-matlab library.","title":"Acknowledgements"},{"location":"surfdata/prac81/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"surfdata/prac81/#wb_view-prevented-from-running-in-windows","text":"In windows 10 you may get a warning dialog stating that Windows protected your PC when you try to run wb_view.exe. If you downloaded this directly from the HCP website then you should be reassured that this is safe. Just click on More Info and then Run anyway .","title":"wb_view prevented from running in Windows"},{"location":"surfdata/prac81/#wb_view-sluggishbehaving-erratically-in-windows","text":"It is possible that your system has an outstanding windows update. Run Windows Update on your computer. Shutdown and restart and then reinstall wb_view to fix this.","title":"wb_view sluggish/behaving erratically  in Windows"},{"location":"surfdata/prac81/#invalid-mex-file-in-mac","text":"When running one or two of the exercises on the Mac you may get an error related to Invalid MEX-file . This is due to an incompatibility between the version of Matlab that you are running, the mac OS version and the compiled version of the mex file that is flagged as in error. It might be possible to solve this in one of three ways: Recompile the mex file in matlab to create a version that is compatible with you mac OS and matlab version Find a compatible mex file online that can act as a drop-in replacement for the problematic version. One good source for replacement mex files is the SPM library . If you are lucky then the mex file in error is one that only provides efficiency gains and is not critical to the functionality of the module. You might be able to simply delete the mex file to carry on with the practicum.","title":"Invalid MEX-file in Mac"},{"location":"surfdata/prac81/#references","text":"Glasser, M. F., Sotiropoulos, S. N., Wilson, J. A., Coalson, T. S., Fischl, B., Andersson, J. L., ... & Van Essen, D. C. (2013). The minimal preprocessing pipelines for the Human Connectome Project. Neuroimage, 80, 105-124. Glasser, M. F., Coalson, T. S., Robinson, E. C., Hacker, C. D., Harwell, J., Yacoub, E., ... & Smith, S. M. (2016). A multi-modal parcellation of human cerebral cortex. Nature, 536(7615), 171-178.","title":"References"}]}